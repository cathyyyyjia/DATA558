{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> DATA 558 Midterm Take-Home Portion </center>\n",
    "<center> Cathy Jia </center>\n",
    "<center> Due May 19, 2019 by 11:59pm </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all of the results.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the gradient $\\nabla F(\\beta)$ of $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the gradient $\\nabla F(\\beta)$ of\n",
    "\n",
    "$$F(\\beta) = \\frac{1}{n}\\sum _{i=1}^n \\frac{1}{\\rho}log(1+exp(-\\rho y_i x_i^T \\beta))+\\lambda\\mid\\mid\\beta\\mid\\mid _2^2$$\n",
    "\n",
    "First, we write $F$ as\n",
    "\n",
    "$$F(\\beta) = A + B \\space, where$$\n",
    "\n",
    "$$A=\\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{\\rho}log(1+exp(-\\rho y_i x_i^T \\beta))$$\n",
    "\n",
    "$$B=\\lambda\\mid\\mid\\beta\\mid\\mid _2^2$$\n",
    "\n",
    "So $\\nabla F(\\beta)$ can be written as\n",
    "\n",
    "$$\\nabla F(\\beta) = \\frac{\\delta A}{\\delta\\beta} + \\frac{\\delta B}{\\delta\\beta}$$\n",
    "\n",
    "We can first find $\\frac{\\delta A}{\\delta\\beta}$.\n",
    "\n",
    "Notice that $A$ can be break down for each $i = 1,2,...,n$ as\n",
    "\n",
    "$$f(g(h(\\beta))), \\space where$$\n",
    "\n",
    "$$h(\\beta) = -\\rho y_i x_i^T \\beta$$\n",
    "\n",
    "$$g(\\beta) = 1 + exp(h(\\beta))$$\n",
    "\n",
    "$$f(\\beta) = \\frac{1}{\\rho}log(g(h(\\beta))$$\n",
    "\n",
    "So we apply Chain rule,\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\beta} \\frac{1}{\\rho}log(1+exp(-\\rho y_i x_i^T \\beta))$$\n",
    "\n",
    "$$= \\frac{1}{\\rho}\\frac{1}{1+exp(-\\rho y_i x_i^T \\beta)} \\frac{\\delta}{\\delta\\beta}(g)$$\n",
    "\n",
    "$$= \\frac{1}{\\rho}\\frac{1}{1+exp(-\\rho y_i x_i^T \\beta)} (0 + exp(-\\rho y_i x_i^T \\beta)) \\frac{\\delta}{\\delta\\beta} (h)$$\n",
    "\n",
    "$$= \\frac{1}{\\rho}\\frac{1}{1+exp(-\\rho y_i x_i^T \\beta)} (0 + exp(-\\rho y_i x_i^T \\beta)) (-\\rho x_i y_i)$$\n",
    "\n",
    "$$= -x_i y_i\\frac{e^{-\\rho y_i x_i^T \\beta}}{1+e^{-\\rho y_i x_i^T \\beta}}$$\n",
    "\n",
    "Define that\n",
    "\n",
    "$$ 1-p_i = \\frac{e^{-\\rho y_i x_i^T \\beta}}{1+e^{-\\rho y_i x_i^T \\beta}} \\space where \\space i = 1,2,...,n$$\n",
    "\n",
    "Then we have\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\beta} \\frac{1}{\\rho}log(1+exp(-\\rho y_i x_i^T \\beta)) = -x_i y_i (1-p_i)$$\n",
    "\n",
    "Therefore, we substitue this into the summation in $A$.\n",
    "\n",
    "$$\\frac{\\delta A}{\\delta\\beta}\n",
    "= \\frac{\\delta}{\\delta\\beta} \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{\\rho}log(1+exp(-\\rho y_i x_i^T \\beta))\n",
    "= -\\frac{1}{n} xyp \\space where\\space p = diag([1-p_1, ..., 1-p_n])$$\n",
    "\n",
    "Then let's find $\\frac{\\delta B}{\\delta\\beta}$.\n",
    "\n",
    "Using the Hammer Identity, for any matrix $A$, $$\\frac{\\delta(X^T AX)}{\\delta X} = (A+A^T)X$$\n",
    "\n",
    "Then we get\n",
    "$$\\frac{\\delta B}{\\delta\\beta}\n",
    "= \\frac{\\delta(\\lambda\\mid\\mid \\beta \\mid\\mid_2^2)}{\\delta\\beta}\n",
    "= \\lambda\\frac{\\delta(\\mid\\mid \\beta \\mid\\mid_2^2)}{\\delta\\beta}\n",
    "= \\lambda\\frac{\\delta(\\beta^T \\beta)}{\\delta\\beta}\n",
    "= \\lambda(I + I^T)\\beta\n",
    "= 2\\lambda\\beta$$\n",
    "\n",
    "Therefore, the gradient $\\nabla F(\\beta)$ of $F$ is\n",
    "\n",
    "$$\\nabla F(\\beta) = \\frac{\\delta A}{\\delta\\beta} + \\frac{\\delta B}{\\delta\\beta} = -\\frac{1}{n}xyp+2\\lambda\\beta$$\n",
    "\n",
    "$$where\\space1-p_i = \\frac{e^{-\\rho y_i x_i^T \\beta}}{1+e^{-\\rho y_i x_i^T \\beta}} , i = 1,2,...,n\\space and\\space p = diag([1-p_1, ..., 1-p_n])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the `Spam` dataset from *The Elements of Statistical Learning*. Standardize the data, if you have not done so already. Be sure to use the training and test splits from the website. You can find the link to the train/test split here: https://web.stanford.edu/~hastie/ElemStatLearn/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 3065\n",
      "Number of dimension: 57\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# Import data\n",
    "spam = pd.read_table('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data',\n",
    "                     sep=' ', header=None)\n",
    "indicator = pd.read_table('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.traintest',\n",
    "                          sep=' ', header=None)\n",
    "spam = spam.dropna()\n",
    "spam_x = np.asarray(spam)[:,:-1]\n",
    "spam_y = np.asarray(spam)[:,-1] * 2 - 1 # change output labels to +/- 1\n",
    "indicator = np.array(indicator).T[0]\n",
    "\n",
    "# Split train data and test data\n",
    "x_train = spam_x[indicator == 0, :]\n",
    "y_train = spam_y[indicator == 0]\n",
    "x_test = spam_x[indicator == 1, :]\n",
    "y_test = spam_y[indicator == 1]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "print('Number of observations: %d' % x_train.shape[0])\n",
    "print('Number of dimension: %d' % x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function `myrhologistic` that implements the accelerated gradient algorithm to train the $l_2$-regularized binary logistic regression with $\\rho$-logistic loss. The function takes as input the initial step-size for the backtracking rule, the $\\epsilon$ for the stopping criterion based on the norm of the gradient of the objective, and the value of $\\rho$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegrad(X, Y, beta, lamb, rho):\n",
    "    n = X.shape[1]\n",
    "    p = np.exp(np.multiply(-Y.T,X.T.dot(beta))*rho)/(1+np.exp(np.multiply(-Y.T,X.T.dot(beta))*rho))\n",
    "    p = np.diag(p)\n",
    "    return -1/n*X.dot(p).dot(Y)+2*lamb*beta\n",
    "\n",
    "def obj(X, Y, beta, lamb, rho):\n",
    "    n = X.shape[1]\n",
    "    return 1/n*np.sum(1/rho*np.log(1+np.exp(np.multiply(-Y.T,X.T.dot(beta))*rho))) + lamb*np.linalg.norm(beta)**2\n",
    "\n",
    "def backtracking(X, Y, beta, grad, init_eta, lamb, rho, max_itr=10):\n",
    "    norm_grad = np.linalg.norm(grad)\n",
    "    eta = init_eta\n",
    "    itr = 0\n",
    "    while itr < max_itr:     \n",
    "        if obj(X, Y, beta-eta*grad, lamb, rho) <= (obj(X, Y, beta, lamb, rho)-0.5*eta*norm_grad**2):\n",
    "            break\n",
    "        else:\n",
    "            eta = 0.8 * eta\n",
    "        itr += 1\n",
    "    return eta\n",
    "\n",
    "def initEta(X, lamb):\n",
    "    n = X.shape[1]\n",
    "    return 1/(max(np.linalg.eigvals(1/n*X.dot(X.T)))+lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myrhologistic(X, Y, init, lamb, rho, eps, max_iter=500):\n",
    "    beta = init\n",
    "    theta = init\n",
    "    # initial step-size value\n",
    "    eta = initEta(X, lamb)\n",
    "    grad = computegrad(X, Y, theta, lamb, rho)\n",
    "    vals = [beta]\n",
    "    t = 0\n",
    "    # stopping criterion: norm(grad) <= eps\n",
    "    while (np.linalg.norm(grad) > eps and t < max_iter):\n",
    "        eta = backtracking(X, Y, beta, grad, eta, lamb, rho)\n",
    "        temp = beta\n",
    "        beta = theta - eta * grad\n",
    "        theta = beta + t/(t+3)*(beta-temp)\n",
    "        vals.append(beta)\n",
    "        grad = computegrad(X, Y, theta, lamb, rho)\n",
    "        t += 1\n",
    "    return np.array(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train your $l_2$-regularized binary logistic regression with $\\rho$-logistic loss with $\\rho = 2$ and $\\epsilon = 10^{−3}$ on the the `Spam` dataset for the $\\lambda = 1$. Report your misclassification error for this value of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, beta, threshold=0.5):\n",
    "    pred = 1/(1+np.exp(-X.T.dot(beta))) > threshold # logistic function\n",
    "    pred = pred.astype(int) # True 1 False 0\n",
    "    pred[pred==0] = class1\n",
    "    pred[pred==1] = class2\n",
    "    return pred.T\n",
    "\n",
    "def compME(X, Y, beta):\n",
    "    pred = predict(X, beta)\n",
    "    err = np.mean(pred != Y)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_T =\n",
      "[ 0.01047308 -0.00746123  0.02713301  0.00999824  0.03380266  0.03268692\n",
      "  0.05330213  0.0302925   0.02764614  0.01634938  0.02985994 -0.00417101\n",
      "  0.01347295  0.00762555  0.02464341  0.05279563  0.03676514  0.02897599\n",
      "  0.0352881   0.02669417  0.0548275   0.01864408  0.04803046  0.0337808\n",
      " -0.032257   -0.02655791 -0.02602812 -0.01751777 -0.01317565 -0.01717074\n",
      " -0.01000275 -0.00710075 -0.01626866 -0.00715029 -0.01235303 -0.01009042\n",
      " -0.02175144 -0.00585375 -0.01806152  0.00057626 -0.01241259 -0.01899183\n",
      " -0.01625808 -0.01406761 -0.02197963 -0.02299674 -0.00692095 -0.01285221\n",
      " -0.00949118 -0.01013322 -0.00786163  0.03162316  0.04787979  0.01071852\n",
      "  0.01437516  0.02498809  0.03192244]\n",
      "\n",
      "The misclassification error for lambda = 1 is 0.0952691680\n"
     ]
    }
   ],
   "source": [
    "vals = myrhologistic(X=x_train.T, Y=y_train, init=np.zeros([x_train.T.shape[0],]), lamb=1, rho=2, eps=0.001)\n",
    "beta_T = vals[vals.shape[0]-1]\n",
    "print('beta_T =')\n",
    "print(beta_T)\n",
    "class1 = -1\n",
    "class2 = 1\n",
    "me = compME(X=x_train.T, Y=y_train, beta=beta_T)\n",
    "print('\\nThe misclassification error for lambda = 1 is %.10f' % me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function `crossval` that implements leave-one-out cross-validation and hold-out cross-validation. You may either write a function that implements each variant separately depending on the case, or write a general cross-validation function that can be instantiated in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "def crossval_split(X, Y, option, train_percent=0):\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    testX = []\n",
    "    testY = []\n",
    "    \n",
    "    # Leave-one-out cross-validation\n",
    "    if option == 'Leave-one-out':\n",
    "        num_split = X.shape[0]\n",
    "        kf = KFold(n_splits=num_split)\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            x_train, x_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "            trainX.append(x_train)\n",
    "            trainY.append(y_train)\n",
    "            testX.append(x_test)\n",
    "            testY.append(y_test)\n",
    "    \n",
    "    # Hold-out cross-validation\n",
    "    elif option == 'Hold-out':\n",
    "        num_split = 1\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=1-train_percent, random_state=1)\n",
    "        trainX.append(x_train)\n",
    "        trainY.append(y_train)\n",
    "        testX.append(x_test)\n",
    "        testY.append(y_test)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('Wrong cross-validation option.')\n",
    "    \n",
    "    print('%s cross-validation' % option)\n",
    "    \n",
    "    return trainX, trainY, testX, testY, num_split\n",
    "\n",
    "\n",
    "def standardize(trainX, testX):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    for i in range(len(trainX)):\n",
    "        scaler = scaler.fit(trainX[i])\n",
    "        trainX[i] = scaler.transform(trainX[i])\n",
    "        testX[i] = scaler.transform(testX[i])\n",
    "    return trainX, testX\n",
    "\n",
    "def crossval_score(trainX, trainY, testX, testY, num_split):\n",
    "    lamb_lst = list(np.arange(0.0, 1.1, 0.1))\n",
    "    scores = []\n",
    "    \n",
    "    print('lambda   average score')\n",
    "    for l in lamb_lst:\n",
    "        score = 0\n",
    "        for i in range(num_split):\n",
    "            vals = myrhologistic(X=trainX[i].T, Y=trainY[i], init=np.zeros([trainX[i].T.shape[0],]),\n",
    "                                 lamb=l, rho=2, eps=0.01)\n",
    "            beta_T = vals[vals.shape[0]-1]\n",
    "            me = compME(X=testX[i].T, Y=testY[i], beta=beta_T)\n",
    "            score += (1 - me)\n",
    "        # compute mean accuracy\n",
    "        acc = score/num_split\n",
    "        scores.append(acc)\n",
    "        print('%.4f   %.10f' % (l, acc))\n",
    "\n",
    "    lamb_opt = lamb_lst[scores.index(max(scores))]\n",
    "    print('\\nThe misclassification error for optimal lambda = %.4f is %.10f'\n",
    "          % (lamb_opt, 1-scores[lamb_lst.index(lamb_opt)]))\n",
    "    \n",
    "    return lamb_opt\n",
    "\n",
    "def crossval(X, Y, option, train_percent=0):\n",
    "    # Hold-out cross-validation\n",
    "    x_train, y_train, x_test, y_test, num_split = crossval_split(X, Y, option, train_percent)\n",
    "\n",
    "    # Standardize the data\n",
    "    x_train, x_test = standardize(x_train, x_test)\n",
    "    \n",
    "    # Compute scores for different lambdas and find the optimal lambda\n",
    "    lamb_opt = crossval_score(x_train, y_train, x_test, y_test, num_split)\n",
    "    \n",
    "    return lamb_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the optimal value of $\\lambda$ using leave-one-out cross-validation. Find the optimal value of $\\lambda$ using hold-out cross-validation with a 80%/20% split for the training set/testing set. Report your misclassification errors for the two values of $\\lambda$ found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leave-one-out cross validation**\n",
    "\n",
    "This is computationally expensive because it requires fitting the logistic model n times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb_opt = crossval(X=spam_x, Y=spam_y, option='Leave-one-out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier and compute misclassification error\n",
    "vals = myrhologistic(X=x_train.T, Y=y_train, init=np.zeros([x_train.T.shape[0],]), lamb=lamb_opt, rho=2, eps=0.001)\n",
    "beta_T = vals[vals.shape[0]-1]\n",
    "print('beta_T =')\n",
    "print(beta_T)\n",
    "me = compME(X=x_train.T, Y=y_train, beta=beta_T)\n",
    "print('\\nThe misclassification error on the original set for optimal lambda = %.4f is %.10f' % (lamb_opt, me))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hold-out cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9294245385\n",
      "0.1000   0.9305103149\n",
      "0.2000   0.9272529859\n",
      "0.3000   0.9250814332\n",
      "0.4000   0.9207383279\n",
      "0.5000   0.9207383279\n",
      "0.6000   0.9207383279\n",
      "0.7000   0.9207383279\n",
      "0.8000   0.9196525516\n",
      "0.9000   0.9185667752\n",
      "1.0000   0.9185667752\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0694896851\n"
     ]
    }
   ],
   "source": [
    "lamb_opt = crossval(X=spam_x, Y=spam_y, option='Hold-out', train_percent=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_T =\n",
      "[ 0.00626555 -0.02616841  0.05437947  0.03946512  0.09501367  0.08099185\n",
      "  0.19977377  0.10152516  0.0684806   0.03485959  0.0544748  -0.0321014\n",
      "  0.01374618  0.017239    0.07485493  0.18784591  0.11673657  0.07569818\n",
      "  0.07395411  0.08206384  0.12991486  0.07691919  0.17372935  0.11245458\n",
      " -0.10194527 -0.06658991 -0.07406637 -0.03441535 -0.02882473 -0.04090532\n",
      " -0.01565605 -0.00644003 -0.0512633  -0.00736901 -0.02471383  0.00187959\n",
      " -0.04332713 -0.02418967 -0.05165912  0.01287898 -0.02951251 -0.06217209\n",
      " -0.03557897 -0.04854555 -0.07858056 -0.07739777 -0.02100094 -0.04260716\n",
      " -0.04321956 -0.02711793 -0.01678827  0.10380603  0.1913603   0.04357385\n",
      "  0.04688199  0.08901197  0.09758823]\n",
      "\n",
      "The misclassification error on the original set for optimal lambda = 0.1000 is 0.0861337684\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier and compute misclassification error\n",
    "vals = myrhologistic(X=x_train.T, Y=y_train, init=np.zeros([x_train.T.shape[0],]), lamb=lamb_opt, rho=2, eps=0.001)\n",
    "beta_T = vals[vals.shape[0]-1]\n",
    "print('beta_T =')\n",
    "print(beta_T)\n",
    "me = compME(X=x_train.T, Y=y_train, beta=beta_T)\n",
    "print('\\nThe misclassification error on the original set for optimal lambda = %.4f is %.10f' % (lamb_opt, me))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick two classes of your choice from the dataset. Train a classifier using $l_2$-regularized binary logistic regression with $\\rho$-logistic loss on the training set using your own accelerated gradient algorithm with $\\rho = 2$, $\\epsilon = 10^{−3}$, and $\\lambda = 1$. Be sure to use the features you previously generated with the provided script rather than the raw image features. Plot, with different colors, the misclassification error on the training set and on the validation set vs iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training subset\n",
      "Number of observations: 1000\n",
      "Number of dimension: 4096\n",
      "\n",
      "Training set\n",
      "Number of observations: 5000\n",
      "Number of dimension: 4096\n",
      "\n",
      "Validation set\n",
      "Number of observations: 1000\n",
      "Number of dimension: 4096\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_train = np.load('train_features.npy')\n",
    "y_train = np.load('train_labels.npy')\n",
    "x_val = np.load('val_features.npy')\n",
    "y_val = np.load('val_labels.npy')\n",
    "\n",
    "# Subset training data: classes 0 and 1\n",
    "class1 = 0\n",
    "class2 = 1\n",
    "xtrain = x_train\n",
    "ytrain = y_train\n",
    "idx_train = np.array([np.where(ytrain==class1),np.where(ytrain==class2)]).reshape(-1)\n",
    "xtrain = xtrain[idx_train]\n",
    "ytrain = ytrain[idx_train]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(np.array(xtrain))\n",
    "xtrain = scaler.transform(xtrain).T\n",
    "scaler = scaler.fit(np.array(x_train))\n",
    "x_train = scaler.transform(x_train).T\n",
    "scaler = scaler.fit(np.array(x_val))\n",
    "x_val = scaler.transform(x_val).T\n",
    "ytrain = ytrain.T\n",
    "y_train = y_train.T\n",
    "y_val = y_val.T\n",
    "\n",
    "# Change label to +/- 1\n",
    "ytrain[ytrain!=class2] = -1\n",
    "\n",
    "print('Training subset')\n",
    "print('Number of observations: %d' % xtrain.shape[1])\n",
    "print('Number of dimension: %d' % xtrain.shape[0])\n",
    "print('\\nTraining set')\n",
    "print('Number of observations: %d' % x_train.shape[1])\n",
    "print('Number of dimension: %d' % x_train.shape[0])\n",
    "print('\\nValidation set')\n",
    "print('Number of observations: %d' % x_val.shape[1])\n",
    "print('Number of dimension: %d' % x_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict(X, beta, cls1, cls2, threshold=0.5):\n",
    "    pred = 1/(1+np.exp(-X.T.dot(beta))) > threshold # logistic function\n",
    "    pred = pred.astype(int) # True 1 False 0\n",
    "    pred[pred==0] = cls1\n",
    "    pred[pred==1] = cls2\n",
    "    return pred.T\n",
    "\n",
    "def compME(X, Y, beta):\n",
    "    pred = predict(X, beta, class1, class2)\n",
    "    err = np.mean(pred != Y)\n",
    "    return err\n",
    "\n",
    "def ME_plot(X1, Y1, X2, Y2, vals):\n",
    "    me1 = []\n",
    "    me2 = []\n",
    "    for val in vals:\n",
    "        me1.append(compME(X=X1, Y=Y1, beta=val))\n",
    "        me2.append(compME(X=X2, Y=Y2, beta=val))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(me1, label='Training Set')\n",
    "    plt.plot(me2, label='Validation Set')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Misclassification Error')\n",
    "    plt.title('Misclassification Error', fontsize=13)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train a classifier\n",
    "vals = myrhologistic(X=xtrain, Y=ytrain, init=np.zeros([xtrain.shape[0],]), lamb=1, rho=2, eps=0.001)\n",
    "\n",
    "# Misclassification error plot\n",
    "ME_plot(x_train, y_train, x_val, y_val, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the value of the regularization parameter $\\lambda$ using leave-one-out cross-validation. Find the value of the regularization parameter $\\lambda$ using hold-out cross-validation. Train a classifier using $l_2$-regularized binary logistic regression with $\\rho$-logistic loss on the training set using your own accelerated gradient algorithm with that value of $\\lambda$ found by hold-out cross-validation. Plot, with different colors, the misclassification error on the training set and on the validation set vs. iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leave-one-out cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = -1\n",
    "class2 = 1\n",
    "lamb_opt = crossval(X=xtrain.T, Y=ytrain, option='Leave-one-out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier\n",
    "vals = myrhologistic(X=xtrain, Y=ytrain, init=np.zeros([xtrain.shape[0],]), lamb=lamb_opt, rho=2, eps=0.001)\n",
    "ME_plot(x_train, y_train, x_val, y_val, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hold-out cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9150000000\n",
      "0.1000   0.9250000000\n",
      "0.2000   0.9250000000\n",
      "0.3000   0.9250000000\n",
      "0.4000   0.9250000000\n",
      "0.5000   0.9250000000\n",
      "0.6000   0.9250000000\n",
      "0.7000   0.9250000000\n",
      "0.8000   0.9200000000\n",
      "0.9000   0.9250000000\n",
      "1.0000   0.9250000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0750000000\n"
     ]
    }
   ],
   "source": [
    "class1 = -1\n",
    "class2 = 1\n",
    "lamb_opt = crossval(X=xtrain.T, Y=ytrain, option='Hold-out', train_percent=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train a classifier\n",
    "vals = myrhologistic(X=xtrain, Y=ytrain, init=np.zeros([xtrain.shape[0],]), lamb=lamb_opt, rho=2, eps=0.001)\n",
    "class1 = 0\n",
    "class2 = 1\n",
    "ME_plot(x_train, y_train, x_val, y_val, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider all pairs of classes from the dataset. For each pair of classes, train a classifier using a $l_2$-regularized binary logistic regression with $\\rho$-logistic loss on the training set comprising only the data-points for that pair of classes using your own fast gradient algorithm. For each pair of classes, find the value of the regularization parameter $\\lambda$ using hold-out cross-validation on the training set comprising only the data-points for that pair of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetData(X, Y):\n",
    "    # Subset training data\n",
    "    x = X\n",
    "    y = Y\n",
    "    idx_train = np.array([np.where(y==class1),np.where(y==class2)]).reshape(-1)\n",
    "    x = x[idx_train]\n",
    "    y = y[idx_train]\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler = scaler.fit(np.array(x))\n",
    "    x = scaler.transform(x).T\n",
    "    y = y.T\n",
    "\n",
    "    # Change label to +/- 1\n",
    "    y[y==class1] = -1\n",
    "    y[y==class2] = 1\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 0\n",
      "Class 2: 1\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9150000000\n",
      "0.1000   0.9250000000\n",
      "0.2000   0.9250000000\n",
      "0.3000   0.9250000000\n",
      "0.4000   0.9250000000\n",
      "0.5000   0.9250000000\n",
      "0.6000   0.9250000000\n",
      "0.7000   0.9250000000\n",
      "0.8000   0.9200000000\n",
      "0.9000   0.9250000000\n",
      "1.0000   0.9250000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0750000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 2\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8800000000\n",
      "0.1000   0.8850000000\n",
      "0.2000   0.8900000000\n",
      "0.3000   0.8850000000\n",
      "0.4000   0.8900000000\n",
      "0.5000   0.8900000000\n",
      "0.6000   0.8900000000\n",
      "0.7000   0.8900000000\n",
      "0.8000   0.8850000000\n",
      "0.9000   0.8850000000\n",
      "1.0000   0.8850000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.2000 is 0.1100000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 3\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9400000000\n",
      "0.1000   0.9400000000\n",
      "0.2000   0.9400000000\n",
      "0.3000   0.9450000000\n",
      "0.4000   0.9450000000\n",
      "0.5000   0.9450000000\n",
      "0.6000   0.9450000000\n",
      "0.7000   0.9450000000\n",
      "0.8000   0.9450000000\n",
      "0.9000   0.9450000000\n",
      "1.0000   0.9450000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.3000 is 0.0550000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 4\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9050000000\n",
      "0.1000   0.9050000000\n",
      "0.2000   0.9100000000\n",
      "0.3000   0.9150000000\n",
      "0.4000   0.9150000000\n",
      "0.5000   0.9200000000\n",
      "0.6000   0.9200000000\n",
      "0.7000   0.9200000000\n",
      "0.8000   0.9200000000\n",
      "0.9000   0.9200000000\n",
      "1.0000   0.9200000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.5000 is 0.0800000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 5\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9300000000\n",
      "0.1000   0.9300000000\n",
      "0.2000   0.9300000000\n",
      "0.3000   0.9300000000\n",
      "0.4000   0.9350000000\n",
      "0.5000   0.9350000000\n",
      "0.6000   0.9350000000\n",
      "0.7000   0.9350000000\n",
      "0.8000   0.9400000000\n",
      "0.9000   0.9400000000\n",
      "1.0000   0.9400000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.8000 is 0.0600000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 6\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9350000000\n",
      "0.1000   0.9350000000\n",
      "0.2000   0.9350000000\n",
      "0.3000   0.9350000000\n",
      "0.4000   0.9350000000\n",
      "0.5000   0.9350000000\n",
      "0.6000   0.9350000000\n",
      "0.7000   0.9350000000\n",
      "0.8000   0.9300000000\n",
      "0.9000   0.9300000000\n",
      "1.0000   0.9300000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0650000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9850000000\n",
      "0.1000   0.9850000000\n",
      "0.2000   0.9850000000\n",
      "0.3000   0.9850000000\n",
      "0.4000   0.9850000000\n",
      "0.5000   0.9850000000\n",
      "0.6000   0.9850000000\n",
      "0.7000   0.9850000000\n",
      "0.8000   0.9850000000\n",
      "0.9000   0.9850000000\n",
      "1.0000   0.9850000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0150000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9650000000\n",
      "0.1000   0.9650000000\n",
      "0.2000   0.9650000000\n",
      "0.3000   0.9650000000\n",
      "0.4000   0.9650000000\n",
      "0.5000   0.9650000000\n",
      "0.6000   0.9650000000\n",
      "0.7000   0.9650000000\n",
      "0.8000   0.9650000000\n",
      "0.9000   0.9650000000\n",
      "1.0000   0.9650000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0350000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9800000000\n",
      "0.1000   0.9800000000\n",
      "0.2000   0.9800000000\n",
      "0.3000   0.9800000000\n",
      "0.4000   0.9800000000\n",
      "0.5000   0.9800000000\n",
      "0.6000   0.9800000000\n",
      "0.7000   0.9800000000\n",
      "0.8000   0.9800000000\n",
      "0.9000   0.9800000000\n",
      "1.0000   0.9800000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0200000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 2\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9000000000\n",
      "0.1000   0.9200000000\n",
      "0.2000   0.9200000000\n",
      "0.3000   0.9100000000\n",
      "0.4000   0.9050000000\n",
      "0.5000   0.9050000000\n",
      "0.6000   0.9050000000\n",
      "0.7000   0.9050000000\n",
      "0.8000   0.9050000000\n",
      "0.9000   0.9050000000\n",
      "1.0000   0.9100000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0800000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 3\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8200000000\n",
      "0.1000   0.8250000000\n",
      "0.2000   0.8250000000\n",
      "0.3000   0.8250000000\n",
      "0.4000   0.8300000000\n",
      "0.5000   0.8400000000\n",
      "0.6000   0.8400000000\n",
      "0.7000   0.8400000000\n",
      "0.8000   0.8400000000\n",
      "0.9000   0.8400000000\n",
      "1.0000   0.8400000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.5000 is 0.1600000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 4\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9000000000\n",
      "0.1000   0.9200000000\n",
      "0.2000   0.9200000000\n",
      "0.3000   0.9200000000\n",
      "0.4000   0.9150000000\n",
      "0.5000   0.9150000000\n",
      "0.6000   0.9150000000\n",
      "0.7000   0.9150000000\n",
      "0.8000   0.9100000000\n",
      "0.9000   0.9100000000\n",
      "1.0000   0.9100000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0800000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 5\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9300000000\n",
      "0.1000   0.9350000000\n",
      "0.2000   0.9250000000\n",
      "0.3000   0.9250000000\n",
      "0.4000   0.9250000000\n",
      "0.5000   0.9250000000\n",
      "0.6000   0.9250000000\n",
      "0.7000   0.9300000000\n",
      "0.8000   0.9300000000\n",
      "0.9000   0.9300000000\n",
      "1.0000   0.9300000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0650000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 6\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9250000000\n",
      "0.1000   0.9300000000\n",
      "0.2000   0.9300000000\n",
      "0.3000   0.9300000000\n",
      "0.4000   0.9350000000\n",
      "0.5000   0.9350000000\n",
      "0.6000   0.9350000000\n",
      "0.7000   0.9350000000\n",
      "0.8000   0.9350000000\n",
      "0.9000   0.9350000000\n",
      "1.0000   0.9350000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.4000 is 0.0650000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9550000000\n",
      "0.1000   0.9550000000\n",
      "0.2000   0.9550000000\n",
      "0.3000   0.9550000000\n",
      "0.4000   0.9550000000\n",
      "0.5000   0.9550000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.6000 is 0.0400000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9500000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9650000000\n",
      "0.3000   0.9650000000\n",
      "0.4000   0.9650000000\n",
      "0.5000   0.9650000000\n",
      "0.6000   0.9650000000\n",
      "0.7000   0.9650000000\n",
      "0.8000   0.9650000000\n",
      "0.9000   0.9650000000\n",
      "1.0000   0.9650000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.2000 is 0.0350000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9400000000\n",
      "0.1000   0.9400000000\n",
      "0.2000   0.9400000000\n",
      "0.3000   0.9400000000\n",
      "0.4000   0.9400000000\n",
      "0.5000   0.9400000000\n",
      "0.6000   0.9400000000\n",
      "0.7000   0.9400000000\n",
      "0.8000   0.9400000000\n",
      "0.9000   0.9400000000\n",
      "1.0000   0.9400000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0600000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 3\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8850000000\n",
      "0.1000   0.8750000000\n",
      "0.2000   0.8750000000\n",
      "0.3000   0.8800000000\n",
      "0.4000   0.8800000000\n",
      "0.5000   0.8800000000\n",
      "0.6000   0.8800000000\n",
      "0.7000   0.8800000000\n",
      "0.8000   0.8800000000\n",
      "0.9000   0.8800000000\n",
      "1.0000   0.8800000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.1150000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 4\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8850000000\n",
      "0.1000   0.8950000000\n",
      "0.2000   0.8850000000\n",
      "0.3000   0.8850000000\n",
      "0.4000   0.8900000000\n",
      "0.5000   0.8900000000\n",
      "0.6000   0.8900000000\n",
      "0.7000   0.8900000000\n",
      "0.8000   0.8900000000\n",
      "0.9000   0.8900000000\n",
      "1.0000   0.8900000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.1050000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 5\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8950000000\n",
      "0.1000   0.9050000000\n",
      "0.2000   0.9050000000\n",
      "0.3000   0.9150000000\n",
      "0.4000   0.9150000000\n",
      "0.5000   0.9150000000\n",
      "0.6000   0.9150000000\n",
      "0.7000   0.9100000000\n",
      "0.8000   0.9050000000\n",
      "0.9000   0.9050000000\n",
      "1.0000   0.9050000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.3000 is 0.0850000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 6\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9450000000\n",
      "0.1000   0.9500000000\n",
      "0.2000   0.9500000000\n",
      "0.3000   0.9500000000\n",
      "0.4000   0.9500000000\n",
      "0.5000   0.9500000000\n",
      "0.6000   0.9500000000\n",
      "0.7000   0.9450000000\n",
      "0.8000   0.9450000000\n",
      "0.9000   0.9450000000\n",
      "1.0000   0.9450000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0500000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda   average score\n",
      "0.0000   0.9650000000\n",
      "0.1000   0.9650000000\n",
      "0.2000   0.9600000000\n",
      "0.3000   0.9600000000\n",
      "0.4000   0.9600000000\n",
      "0.5000   0.9600000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0350000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9550000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9600000000\n",
      "0.3000   0.9600000000\n",
      "0.4000   0.9600000000\n",
      "0.5000   0.9600000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0400000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9350000000\n",
      "0.1000   0.9350000000\n",
      "0.2000   0.9350000000\n",
      "0.3000   0.9350000000\n",
      "0.4000   0.9350000000\n",
      "0.5000   0.9350000000\n",
      "0.6000   0.9400000000\n",
      "0.7000   0.9400000000\n",
      "0.8000   0.9400000000\n",
      "0.9000   0.9400000000\n",
      "1.0000   0.9400000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.6000 is 0.0600000000\n",
      "\n",
      "\n",
      "Class 1: 3\n",
      "Class 2: 4\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9050000000\n",
      "0.1000   0.9050000000\n",
      "0.2000   0.9050000000\n",
      "0.3000   0.9050000000\n",
      "0.4000   0.9050000000\n",
      "0.5000   0.9050000000\n",
      "0.6000   0.9050000000\n",
      "0.7000   0.9050000000\n",
      "0.8000   0.9050000000\n",
      "0.9000   0.9050000000\n",
      "1.0000   0.9100000000\n",
      "\n",
      "The misclassification error for optimal lambda = 1.0000 is 0.0900000000\n",
      "\n",
      "\n",
      "Class 1: 3\n",
      "Class 2: 5\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9500000000\n",
      "0.1000   0.9550000000\n",
      "0.2000   0.9550000000\n",
      "0.3000   0.9550000000\n",
      "0.4000   0.9550000000\n",
      "0.5000   0.9550000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.6000 is 0.0400000000\n",
      "\n",
      "\n",
      "Class 1: 3\n",
      "Class 2: 6\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8500000000\n",
      "0.1000   0.8500000000\n",
      "0.2000   0.8350000000\n",
      "0.3000   0.8350000000\n",
      "0.4000   0.8350000000\n",
      "0.5000   0.8350000000\n",
      "0.6000   0.8300000000\n",
      "0.7000   0.8250000000\n",
      "0.8000   0.8250000000\n",
      "0.9000   0.8300000000\n",
      "1.0000   0.8350000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.1500000000\n",
      "\n",
      "\n",
      "Class 1: 3\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9300000000\n",
      "0.1000   0.9450000000\n",
      "0.2000   0.9450000000\n",
      "0.3000   0.9450000000\n",
      "0.4000   0.9450000000\n",
      "0.5000   0.9450000000\n",
      "0.6000   0.9450000000\n",
      "0.7000   0.9450000000\n",
      "0.8000   0.9450000000\n",
      "0.9000   0.9450000000\n",
      "1.0000   0.9450000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0550000000\n",
      "\n",
      "\n",
      "Class 1: 3\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9550000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9600000000\n",
      "0.3000   0.9600000000\n",
      "0.4000   0.9600000000\n",
      "0.5000   0.9600000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0400000000\n",
      "\n",
      "\n",
      "Class 1: 3\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9050000000\n",
      "0.1000   0.9100000000\n",
      "0.2000   0.9100000000\n",
      "0.3000   0.9100000000\n",
      "0.4000   0.9100000000\n",
      "0.5000   0.9100000000\n",
      "0.6000   0.9150000000\n",
      "0.7000   0.9150000000\n",
      "0.8000   0.9100000000\n",
      "0.9000   0.9100000000\n",
      "1.0000   0.9100000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.6000 is 0.0850000000\n",
      "\n",
      "\n",
      "Class 1: 4\n",
      "Class 2: 5\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8800000000\n",
      "0.1000   0.8850000000\n",
      "0.2000   0.8850000000\n",
      "0.3000   0.8850000000\n",
      "0.4000   0.8850000000\n",
      "0.5000   0.8850000000\n",
      "0.6000   0.8850000000\n",
      "0.7000   0.8850000000\n",
      "0.8000   0.8800000000\n",
      "0.9000   0.8700000000\n",
      "1.0000   0.8700000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.1150000000\n",
      "\n",
      "\n",
      "Class 1: 4\n",
      "Class 2: 6\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9650000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9750000000\n",
      "0.3000   0.9750000000\n",
      "0.4000   0.9750000000\n",
      "0.5000   0.9750000000\n",
      "0.6000   0.9750000000\n",
      "0.7000   0.9750000000\n",
      "0.8000   0.9750000000\n",
      "0.9000   0.9750000000\n",
      "1.0000   0.9750000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.2000 is 0.0250000000\n",
      "\n",
      "\n",
      "Class 1: 4\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9800000000\n",
      "0.1000   0.9800000000\n",
      "0.2000   0.9800000000\n",
      "0.3000   0.9800000000\n",
      "0.4000   0.9800000000\n",
      "0.5000   0.9800000000\n",
      "0.6000   0.9800000000\n",
      "0.7000   0.9800000000\n",
      "0.8000   0.9800000000\n",
      "0.9000   0.9800000000\n",
      "1.0000   0.9800000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0200000000\n",
      "\n",
      "\n",
      "Class 1: 4\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9750000000\n",
      "0.1000   0.9700000000\n",
      "0.2000   0.9700000000\n",
      "0.3000   0.9700000000\n",
      "0.4000   0.9700000000\n",
      "0.5000   0.9700000000\n",
      "0.6000   0.9700000000\n",
      "0.7000   0.9700000000\n",
      "0.8000   0.9700000000\n",
      "0.9000   0.9700000000\n",
      "1.0000   0.9700000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0250000000\n",
      "\n",
      "\n",
      "Class 1: 4\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9600000000\n",
      "0.1000   0.9700000000\n",
      "0.2000   0.9700000000\n",
      "0.3000   0.9700000000\n",
      "0.4000   0.9700000000\n",
      "0.5000   0.9700000000\n",
      "0.6000   0.9650000000\n",
      "0.7000   0.9650000000\n",
      "0.8000   0.9650000000\n",
      "0.9000   0.9650000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0300000000\n",
      "\n",
      "\n",
      "Class 1: 5\n",
      "Class 2: 6\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9600000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9600000000\n",
      "0.3000   0.9650000000\n",
      "0.4000   0.9650000000\n",
      "0.5000   0.9650000000\n",
      "0.6000   0.9650000000\n",
      "0.7000   0.9650000000\n",
      "0.8000   0.9650000000\n",
      "0.9000   0.9650000000\n",
      "1.0000   0.9650000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.3000 is 0.0350000000\n",
      "\n",
      "\n",
      "Class 1: 5\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9800000000\n",
      "0.1000   0.9800000000\n",
      "0.2000   0.9800000000\n",
      "0.3000   0.9800000000\n",
      "0.4000   0.9800000000\n",
      "0.5000   0.9800000000\n",
      "0.6000   0.9800000000\n",
      "0.7000   0.9800000000\n",
      "0.8000   0.9800000000\n",
      "0.9000   0.9800000000\n",
      "1.0000   0.9800000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0200000000\n",
      "\n",
      "\n",
      "Class 1: 5\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9750000000\n",
      "0.1000   0.9650000000\n",
      "0.2000   0.9550000000\n",
      "0.3000   0.9550000000\n",
      "0.4000   0.9550000000\n",
      "0.5000   0.9550000000\n",
      "0.6000   0.9550000000\n",
      "0.7000   0.9550000000\n",
      "0.8000   0.9550000000\n",
      "0.9000   0.9550000000\n",
      "1.0000   0.9550000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0250000000\n",
      "\n",
      "\n",
      "Class 1: 5\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9600000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9600000000\n",
      "0.3000   0.9600000000\n",
      "0.4000   0.9600000000\n",
      "0.5000   0.9600000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0400000000\n",
      "\n",
      "\n",
      "Class 1: 6\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9600000000\n",
      "0.1000   0.9650000000\n",
      "0.2000   0.9700000000\n",
      "0.3000   0.9700000000\n",
      "0.4000   0.9700000000\n",
      "0.5000   0.9700000000\n",
      "0.6000   0.9700000000\n",
      "0.7000   0.9700000000\n",
      "0.8000   0.9700000000\n",
      "0.9000   0.9700000000\n",
      "1.0000   0.9700000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.2000 is 0.0300000000\n",
      "\n",
      "\n",
      "Class 1: 6\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9900000000\n",
      "0.1000   0.9950000000\n",
      "0.2000   0.9950000000\n",
      "0.3000   0.9950000000\n",
      "0.4000   0.9950000000\n",
      "0.5000   0.9950000000\n",
      "0.6000   0.9950000000\n",
      "0.7000   0.9950000000\n",
      "0.8000   0.9950000000\n",
      "0.9000   0.9950000000\n",
      "1.0000   0.9950000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0050000000\n",
      "\n",
      "\n",
      "Class 1: 6\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9600000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9600000000\n",
      "0.3000   0.9600000000\n",
      "0.4000   0.9600000000\n",
      "0.5000   0.9600000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0400000000\n",
      "\n",
      "\n",
      "Class 1: 7\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8250000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1000   0.8450000000\n",
      "0.2000   0.8500000000\n",
      "0.3000   0.8450000000\n",
      "0.4000   0.8450000000\n",
      "0.5000   0.8500000000\n",
      "0.6000   0.8550000000\n",
      "0.7000   0.8500000000\n",
      "0.8000   0.8500000000\n",
      "0.9000   0.8550000000\n",
      "1.0000   0.8500000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.6000 is 0.1450000000\n",
      "\n",
      "\n",
      "Class 1: 7\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8450000000\n",
      "0.1000   0.8500000000\n",
      "0.2000   0.8550000000\n",
      "0.3000   0.8500000000\n",
      "0.4000   0.8450000000\n",
      "0.5000   0.8500000000\n",
      "0.6000   0.8450000000\n",
      "0.7000   0.8500000000\n",
      "0.8000   0.8550000000\n",
      "0.9000   0.8500000000\n",
      "1.0000   0.8500000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.2000 is 0.1450000000\n",
      "\n",
      "\n",
      "Class 1: 8\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8750000000\n",
      "0.1000   0.8700000000\n",
      "0.2000   0.8700000000\n",
      "0.3000   0.8700000000\n",
      "0.4000   0.8700000000\n",
      "0.5000   0.8700000000\n",
      "0.6000   0.8650000000\n",
      "0.7000   0.8650000000\n",
      "0.8000   0.8700000000\n",
      "0.9000   0.8750000000\n",
      "1.0000   0.8750000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.1250000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load train set and validation set\n",
    "x_train = np.load('train_features.npy')\n",
    "y_train = np.load('train_labels.npy').T\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "pairs = list(combinations([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 2))\n",
    "clfs = []\n",
    "for pair in pairs:\n",
    "    class1 = pair[0]\n",
    "    class2 = pair[1]\n",
    "    print('Class 1: %d\\nClass 2: %d' % (class1, class2))\n",
    "    xtrain, ytrain = subsetData(x_train, y_train)\n",
    "    class1 = -1\n",
    "    class2 = 1\n",
    "    # Leave-one-out cross validation\n",
    "    #lamb_opt = crossval(X=xtrain.T, Y=ytrain, option='Leave-one-out')\n",
    "    # Hold-out cross validation\n",
    "    lamb_opt = crossval(X=xtrain.T, Y=ytrain, option='Hold-out', train_percent=0.8)\n",
    "    print('')\n",
    "    # Train a classifier\n",
    "    vals = myrhologistic(X=xtrain, Y=ytrain, init=np.zeros([xtrain.shape[0],]), lamb=lamb_opt, rho=2, eps=0.001)\n",
    "    clfs.append(vals[vals.shape[0]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function that for any new data point predicts its label. To do this, you will perform the following: input the data point into each classifier (for each pair of classes) you trained above. Record the class predicted by each classifier. Then your prediction for this data point is the most frequently predicted class. If there is a tie, randomly choose between the tied classes. Report the misclassification error on the validation set and test set. Report the precision/recall on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict Label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def predict(X, beta, cls1, cls2, threshold=0.5):\n",
    "    pred = 1/(1+np.exp(-X.T.dot(beta))) > threshold # logistic function\n",
    "    pred = pred.astype(int) # True 1 False 0\n",
    "    pred[pred==0] = cls1\n",
    "    pred[pred==1] = cls2\n",
    "    return pred.T\n",
    "\n",
    "def prediction(X, betas):\n",
    "    preds = np.array([[0]]*X.shape[1])\n",
    "    \n",
    "    for i in range(len(betas)):\n",
    "        pred = predict(X, betas[i], pairs[i][0], pairs[i][1])\n",
    "        pred = np.array([pred])\n",
    "        preds = np.concatenate((preds, pred.T), axis=1)\n",
    "    res, cnt = mode(preds, axis=1)\n",
    "    \n",
    "    return res, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification error on the validation set: 0.3870000000\n",
      "Precision on the validation set: 0.6130000000\n"
     ]
    }
   ],
   "source": [
    "# Load and standardize validation set \n",
    "x_val = np.load('val_features.npy')\n",
    "y_val = np.load('val_labels.npy').T\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(np.array(x_val))\n",
    "x_val = scaler.transform(x_val).T\n",
    "\n",
    "pred, preds = prediction(x_val, clfs)\n",
    "pred = pred.reshape(-1)\n",
    "\n",
    "me_val = np.mean(pred != y_val)\n",
    "print('Misclassification error on the validation set: %.10f' % me_val)\n",
    "print('Precision on the validation set: %.10f' % (1-me_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and standardize test set \n",
    "x_test = np.load('test_features.npy')\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(x_test)\n",
    "x_test = scaler.transform(x_test).T\n",
    "\n",
    "# Predict\n",
    "pred, preds = prediction(x_test, clfs)\n",
    "pred = pred.reshape(-1)\n",
    "\n",
    "# Write to submission.csv\n",
    "df = pd.read_csv('sample_submission.csv')\n",
    "df['Category'] = pred\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification error on the test set: 0.4033400000\n"
     ]
    }
   ],
   "source": [
    "me_test = 1 - 0.59666 # score from Kaggle\n",
    "print('Misclassification error on the test set: %.10f' % me_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
