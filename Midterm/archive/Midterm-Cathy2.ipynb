{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> DATA 558 Midterm Take-Home Portion </center>\n",
    "<center> Cathy Jia </center>\n",
    "<center> Due May 19, 2019 by 11:59pm </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all of the results.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the gradient $\\nabla F(\\beta)$ of $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the gradient $\\nabla F(\\beta)$ of\n",
    "\n",
    "$$F(\\beta) = \\frac{1}{n}\\sum _{i=1}^n \\frac{1}{\\rho}log(1+exp(-\\rho y_i x_i^T \\beta))+\\lambda\\mid\\mid\\beta\\mid\\mid _2^2$$\n",
    "\n",
    "First, we write $F$ as\n",
    "\n",
    "$$F(\\beta) = A + B \\space, where$$\n",
    "\n",
    "$$A=\\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{\\rho}log(1+exp(-\\rho y_i x_i^T \\beta))$$\n",
    "\n",
    "$$B=\\lambda\\mid\\mid\\beta\\mid\\mid _2^2$$\n",
    "\n",
    "So $\\nabla F(\\beta)$ can be written as\n",
    "\n",
    "$$\\nabla F(\\beta) = \\frac{\\delta A}{\\delta\\beta} + \\frac{\\delta B}{\\delta\\beta}$$\n",
    "\n",
    "We can first find $\\frac{\\delta A}{\\delta\\beta}$.\n",
    "\n",
    "Notice that $A$ can be break down for each $i = 1,2,...,n$ as\n",
    "\n",
    "$$f(g(h(\\beta))), \\space where$$\n",
    "\n",
    "$$h(\\beta) = -\\rho y_i x_i^T \\beta$$\n",
    "\n",
    "$$g(\\beta) = 1 + exp(h(\\beta))$$\n",
    "\n",
    "$$f(\\beta) = \\frac{1}{\\rho}log(g(h(\\beta))$$\n",
    "\n",
    "So we apply Chain rule,\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\beta} \\frac{1}{\\rho}log(1+exp(-\\rho y_i x_i^T \\beta))$$\n",
    "\n",
    "$$= \\frac{1}{\\rho}\\frac{1}{1+exp(-\\rho y_i x_i^T \\beta)} \\frac{\\delta}{\\delta\\beta}(g)$$\n",
    "\n",
    "$$= \\frac{1}{\\rho}\\frac{1}{1+exp(-\\rho y_i x_i^T \\beta)} (0 + exp(-\\rho y_i x_i^T \\beta)) \\frac{\\delta}{\\delta\\beta} (h)$$\n",
    "\n",
    "$$= \\frac{1}{\\rho}\\frac{1}{1+exp(-\\rho y_i x_i^T \\beta)} (0 + exp(-\\rho y_i x_i^T \\beta)) (-\\rho x_i y_i)$$\n",
    "\n",
    "$$= -x_i y_i\\frac{e^{-\\rho y_i x_i^T \\beta}}{1+e^{-\\rho y_i x_i^T \\beta}}$$\n",
    "\n",
    "Define that\n",
    "\n",
    "$$ 1-p_i = \\frac{e^{-\\rho y_i x_i^T \\beta}}{1+e^{-\\rho y_i x_i^T \\beta}} \\space where \\space i = 1,2,...,n$$\n",
    "\n",
    "Then we have\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\beta} \\frac{1}{\\rho}log(1+exp(-\\rho y_i x_i^T \\beta)) = -x_i y_i (1-p_i)$$\n",
    "\n",
    "Therefore, we substitue this into the summation in $A$.\n",
    "\n",
    "$$\\frac{\\delta A}{\\delta\\beta}\n",
    "= \\frac{\\delta}{\\delta\\beta} \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{\\rho}log(1+exp(-\\rho y_i x_i^T \\beta))\n",
    "= -\\frac{1}{n} xyp \\space where\\space p = diag([1-p_1, ..., 1-p_n])$$\n",
    "\n",
    "Then let's find $\\frac{\\delta B}{\\delta\\beta}$.\n",
    "\n",
    "Using the Hammer Identity, for any matrix $A$, $$\\frac{\\delta(X^T AX)}{\\delta X} = (A+A^T)X$$\n",
    "\n",
    "Then we get\n",
    "$$\\frac{\\delta B}{\\delta\\beta}\n",
    "= \\frac{\\delta(\\lambda\\mid\\mid \\beta \\mid\\mid_2^2)}{\\delta\\beta}\n",
    "= \\lambda\\frac{\\delta(\\mid\\mid \\beta \\mid\\mid_2^2)}{\\delta\\beta}\n",
    "= \\lambda\\frac{\\delta(\\beta^T \\beta)}{\\delta\\beta}\n",
    "= \\lambda(I + I^T)\\beta\n",
    "= 2\\lambda\\beta$$\n",
    "\n",
    "Therefore, the gradient $\\nabla F(\\beta)$ of $F$ is\n",
    "\n",
    "$$\\nabla F(\\beta) = \\frac{\\delta A}{\\delta\\beta} + \\frac{\\delta B}{\\delta\\beta} = -\\frac{1}{n}xyp+2\\lambda\\beta$$\n",
    "\n",
    "$$where\\space1-p_i = \\frac{e^{-\\rho y_i x_i^T \\beta}}{1+e^{-\\rho y_i x_i^T \\beta}} , i = 1,2,...,n\\space and\\space p = diag([1-p_1, ..., 1-p_n])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the `Spam` dataset from *The Elements of Statistical Learning*. Standardize the data, if you have not done so already. Be sure to use the training and test splits from the website. You can find the link to the train/test split here: https://web.stanford.edu/~hastie/ElemStatLearn/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 3065\n",
      "Number of dimension: 57\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# Import data\n",
    "spam = pd.read_table('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data',\n",
    "                     sep=' ', header=None)\n",
    "indicator = pd.read_table('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.traintest',\n",
    "                          sep=' ', header=None)\n",
    "spam = spam.dropna()\n",
    "spam_x = np.asarray(spam)[:,:-1]\n",
    "spam_y = np.asarray(spam)[:,-1] * 2 - 1 # change output labels to +/- 1\n",
    "indicator = np.array(indicator).T[0]\n",
    "\n",
    "# Split train data and test data\n",
    "x_train = spam_x[indicator == 0, :]\n",
    "y_train = spam_y[indicator == 0]\n",
    "x_test = spam_x[indicator == 1, :]\n",
    "y_test = spam_y[indicator == 1]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "print('Number of observations: %d' % x_train.shape[0])\n",
    "print('Number of dimension: %d' % x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function `myrhologistic` that implements the accelerated gradient algorithm to train the $l_2$-regularized binary logistic regression with $\\rho$-logistic loss. The function takes as input the initial step-size for the backtracking rule, the $\\epsilon$ for the stopping criterion based on the norm of the gradient of the objective, and the value of $\\rho$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegrad(X, Y, beta, lamb, rho):\n",
    "    n = X.shape[1]\n",
    "    p = np.exp(np.multiply(-Y.T,X.T.dot(beta))*rho)/(1+np.exp(np.multiply(-Y.T,X.T.dot(beta))*rho))\n",
    "    p = np.diag(p)\n",
    "    return -1/n*X.dot(p).dot(Y)+2*lamb*beta\n",
    "\n",
    "def obj(X, Y, beta, lamb, rho):\n",
    "    n = X.shape[1]\n",
    "    return 1/n*np.sum(1/rho*np.log(1+np.exp(np.multiply(-Y.T,X.T.dot(beta))*rho))) + lamb*np.linalg.norm(beta)**2\n",
    "\n",
    "def backtracking(X, Y, beta, grad, init_eta, lamb, rho, max_itr=10):\n",
    "    norm_grad = np.linalg.norm(grad)\n",
    "    eta = init_eta\n",
    "    itr = 0\n",
    "    while itr < max_itr:     \n",
    "        if obj(X, Y, beta-eta*grad, lamb, rho) <= (obj(X, Y, beta, lamb, rho)-0.5*eta*norm_grad**2):\n",
    "            break\n",
    "        else:\n",
    "            eta = 0.8 * eta\n",
    "        itr += 1\n",
    "    return eta\n",
    "\n",
    "def initEta(X, lamb):\n",
    "    n = X.shape[1]\n",
    "    return 1/(max(np.linalg.eigvals(1/n*X.dot(X.T)))+lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myrhologistic(X, Y, init, lamb, rho, eps, max_iter=500):\n",
    "    beta = init\n",
    "    theta = init\n",
    "    # initial step-size value\n",
    "    eta = initEta(X, lamb)\n",
    "    grad = computegrad(X, Y, theta, lamb, rho)\n",
    "    vals = [beta]\n",
    "    t = 0\n",
    "    # stopping criterion: norm(grad) <= eps\n",
    "    while (np.linalg.norm(grad) > eps and t < max_iter):\n",
    "        eta = backtracking(X, Y, beta, grad, eta, lamb, rho)\n",
    "        temp = beta\n",
    "        beta = theta - eta * grad\n",
    "        theta = beta + t/(t+3)*(beta-temp)\n",
    "        vals.append(beta)\n",
    "        grad = computegrad(X, Y, theta, lamb, rho)\n",
    "        t += 1\n",
    "    return np.array(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train your $l_2$-regularized binary logistic regression with $\\rho$-logistic loss with $\\rho = 2$ and $\\epsilon = 10^{âˆ’3}$ on the the `Spam` dataset for the $\\lambda = 1$. Report your misclassification error for this value of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, beta, threshold=0.5):\n",
    "    pred = 1/(1+np.exp(-X.T.dot(beta))) > threshold # logistic function\n",
    "    pred = pred.astype(int) # True 1 False 0\n",
    "    pred[pred==0] = class1\n",
    "    pred[pred==1] = class2\n",
    "    return pred.T\n",
    "\n",
    "def compME(X, Y, beta):\n",
    "    pred = predict(X, beta)\n",
    "    err = np.mean(pred != Y)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_T =\n",
      "[ 0.01047308 -0.00746123  0.02713301  0.00999824  0.03380266  0.03268692\n",
      "  0.05330213  0.0302925   0.02764614  0.01634938  0.02985994 -0.00417101\n",
      "  0.01347295  0.00762555  0.02464341  0.05279563  0.03676514  0.02897599\n",
      "  0.0352881   0.02669417  0.0548275   0.01864408  0.04803046  0.0337808\n",
      " -0.032257   -0.02655791 -0.02602812 -0.01751777 -0.01317565 -0.01717074\n",
      " -0.01000275 -0.00710075 -0.01626866 -0.00715029 -0.01235303 -0.01009042\n",
      " -0.02175144 -0.00585375 -0.01806152  0.00057626 -0.01241259 -0.01899183\n",
      " -0.01625808 -0.01406761 -0.02197963 -0.02299674 -0.00692095 -0.01285221\n",
      " -0.00949118 -0.01013322 -0.00786163  0.03162316  0.04787979  0.01071852\n",
      "  0.01437516  0.02498809  0.03192244]\n",
      "\n",
      "The misclassification error for lambda = 1 is 0.0952691680\n"
     ]
    }
   ],
   "source": [
    "vals = myrhologistic(X=x_train.T, Y=y_train, init=np.zeros([x_train.T.shape[0],]), lamb=1, rho=2, eps=0.001)\n",
    "beta_T = vals[vals.shape[0]-1]\n",
    "print('beta_T =')\n",
    "print(beta_T)\n",
    "class1 = -1\n",
    "class2 = 1\n",
    "me = compME(X=x_train.T, Y=y_train, beta=beta_T)\n",
    "print('\\nThe misclassification error for lambda = 1 is %.10f' % me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function `crossval` that implements leave-one-out cross-validation and hold-out cross-validation. You may either write a function that implements each variant separately depending on the case, or write a general cross-validation function that can be instantiated in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "def crossval_split(X, Y, option, train_percent=0):\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    testX = []\n",
    "    testY = []\n",
    "    \n",
    "    # Leave-one-out cross-validation\n",
    "    if option == 'Leave-one-out':\n",
    "        num_split = X.shape[0]\n",
    "        kf = KFold(n_splits=num_split)\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            x_train, x_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "            trainX.append(x_train)\n",
    "            trainY.append(y_train)\n",
    "            testX.append(x_test)\n",
    "            testY.append(y_test)\n",
    "    \n",
    "    # Hold-out cross-validation\n",
    "    elif option == 'Hold-out':\n",
    "        num_split = 1\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=1-train_percent, random_state=1)\n",
    "        trainX.append(x_train)\n",
    "        trainY.append(y_train)\n",
    "        testX.append(x_test)\n",
    "        testY.append(y_test)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('Wrong cross-validation option.')\n",
    "    \n",
    "    print('%s cross-validation' % option)\n",
    "    \n",
    "    return trainX, trainY, testX, testY, num_split\n",
    "\n",
    "\n",
    "def standardize(trainX, testX):\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    for i in range(len(trainX)):\n",
    "        scaler = scaler.fit(trainX[i])\n",
    "        trainX[i] = scaler.transform(trainX[i])\n",
    "        scaler = scaler.fit(testX[i])\n",
    "        testX[i] = scaler.transform(testX[i])\n",
    "    return trainX, testX\n",
    "\n",
    "def crossval_score(trainX, trainY, testX, testY, num_split):\n",
    "    lamb_lst = list(np.arange(0.0, 1.1, 0.1))\n",
    "    scores = []\n",
    "    \n",
    "    print('lambda   average score')\n",
    "    for l in lamb_lst:\n",
    "        score = 0\n",
    "        for i in range(num_split):\n",
    "            vals = myrhologistic(X=trainX[i].T, Y=trainY[i], init=np.zeros([trainX[i].T.shape[0],]),\n",
    "                                 lamb=l, rho=2, eps=0.01)\n",
    "            beta_T = vals[vals.shape[0]-1]\n",
    "            me = compME(X=testX[i].T, Y=testY[i], beta=beta_T)\n",
    "            score += (1 - me)\n",
    "        # compute mean accuracy\n",
    "        acc = score/num_split\n",
    "        scores.append(acc)\n",
    "        print('%.4f   %.10f' % (l, acc))\n",
    "\n",
    "    lamb_opt = lamb_lst[scores.index(max(scores))]\n",
    "    print('\\nThe misclassification error for optimal lambda = %.4f is %.10f'\n",
    "          % (lamb_opt, 1-scores[lamb_lst.index(lamb_opt)]))\n",
    "    \n",
    "    return lamb_opt\n",
    "\n",
    "def crossval(X, Y, option, train_percent=0):\n",
    "    # Hold-out cross-validation\n",
    "    x_train, y_train, x_test, y_test, num_split = crossval_split(X, Y, option, train_percent)\n",
    "\n",
    "    # Standardize the data\n",
    "    x_train, x_test = standardize(x_train, x_test)\n",
    "    \n",
    "    # Compute scores for different lambdas and find the optimal lambda\n",
    "    lamb_opt = crossval_score(x_train, y_train, x_test, y_test, num_split)\n",
    "    \n",
    "    return lamb_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the optimal value of $\\lambda$ using leave-one-out cross-validation. Find the optimal value of $\\lambda$ using hold-out cross-validation with a 80%/20% split for the training set/testing set. Report your misclassification errors for the two values of $\\lambda$ found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leave-one-out cross validation**\n",
    "\n",
    "This is computationally expensive because it requires fitting the logistic model n times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb_opt = crossval(X=spam_x, Y=spam_y, option='Leave-one-out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_T =\n",
      "[ 0.01182409 -0.01584126  0.04437356  0.02195192  0.06490424  0.05895461\n",
      "  0.11342058  0.06189994  0.04898052  0.0276981   0.0486688  -0.01559467\n",
      "  0.01796644  0.01263342  0.04549507  0.1107973   0.07180218  0.0540647\n",
      "  0.05801683  0.05094264  0.09849249  0.04277602  0.0985633   0.06802872\n",
      " -0.06057523 -0.04533175 -0.0487055  -0.0274812  -0.02112287 -0.0286681\n",
      " -0.01351449 -0.0078975  -0.03160926 -0.00812683 -0.01902813 -0.00966068\n",
      " -0.03501302 -0.0135115  -0.0335539   0.00622427 -0.02112117 -0.03736114\n",
      " -0.02669049 -0.02833849 -0.04566443 -0.04618058 -0.01352151 -0.02548431\n",
      " -0.02187473 -0.01800499 -0.0126132   0.06362409  0.10287253  0.0232627\n",
      "  0.02818843  0.0503403   0.06179635]\n",
      "\n",
      "The misclassification error on the original set for optimal lambda = 0.3000 is 0.0874388254\n"
     ]
    }
   ],
   "source": [
    "# Run Leave-one-out cross validation on AWS overnight\n",
    "lamb_opt = 0.3\n",
    "\n",
    "# Train a classifier and compute misclassification error\n",
    "vals = myrhologistic(X=x_train.T, Y=y_train, init=np.zeros([x_train.T.shape[0],]), lamb=lamb_opt, rho=2, eps=0.001)\n",
    "beta_T = vals[vals.shape[0]-1]\n",
    "print('beta_T =')\n",
    "print(beta_T)\n",
    "me = compME(X=x_train.T, Y=y_train, beta=beta_T)\n",
    "print('\\nThe misclassification error on the original set for optimal lambda = %.4f is %.10f' % (lamb_opt, me))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hold-out cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9272529859\n",
      "0.1000   0.9283387622\n",
      "0.2000   0.9272529859\n",
      "0.3000   0.9261672096\n",
      "0.4000   0.9239956569\n",
      "0.5000   0.9196525516\n",
      "0.6000   0.9207383279\n",
      "0.7000   0.9196525516\n",
      "0.8000   0.9196525516\n",
      "0.9000   0.9185667752\n",
      "1.0000   0.9185667752\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0716612378\n"
     ]
    }
   ],
   "source": [
    "lamb_opt = crossval(X=spam_x, Y=spam_y, option='Hold-out', train_percent=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_T =\n",
      "[ 0.00626555 -0.02616841  0.05437947  0.03946512  0.09501367  0.08099185\n",
      "  0.19977377  0.10152516  0.0684806   0.03485959  0.0544748  -0.0321014\n",
      "  0.01374618  0.017239    0.07485493  0.18784591  0.11673657  0.07569818\n",
      "  0.07395411  0.08206384  0.12991486  0.07691919  0.17372935  0.11245458\n",
      " -0.10194527 -0.06658991 -0.07406637 -0.03441535 -0.02882473 -0.04090532\n",
      " -0.01565605 -0.00644003 -0.0512633  -0.00736901 -0.02471383  0.00187959\n",
      " -0.04332713 -0.02418967 -0.05165912  0.01287898 -0.02951251 -0.06217209\n",
      " -0.03557897 -0.04854555 -0.07858056 -0.07739777 -0.02100094 -0.04260716\n",
      " -0.04321956 -0.02711793 -0.01678827  0.10380603  0.1913603   0.04357385\n",
      "  0.04688199  0.08901197  0.09758823]\n",
      "\n",
      "The misclassification error on the original set for optimal lambda = 0.1000 is 0.0861337684\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier and compute misclassification error\n",
    "vals = myrhologistic(X=x_train.T, Y=y_train, init=np.zeros([x_train.T.shape[0],]), lamb=lamb_opt, rho=2, eps=0.001)\n",
    "beta_T = vals[vals.shape[0]-1]\n",
    "print('beta_T =')\n",
    "print(beta_T)\n",
    "me = compME(X=x_train.T, Y=y_train, beta=beta_T)\n",
    "print('\\nThe misclassification error on the original set for optimal lambda = %.4f is %.10f' % (lamb_opt, me))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick two classes of your choice from the dataset. Train a classifier using $l_2$-regularized binary logistic regression with $\\rho$-logistic loss on the training set using your own accelerated gradient algorithm with $\\rho = 2$, $\\epsilon = 10^{âˆ’3}$, and $\\lambda = 1$. Be sure to use the features you previously generated with the provided script rather than the raw image features. Plot, with different colors, the misclassification error on the training set and on the validation set vs iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training subset\n",
      "Number of observations: 1000\n",
      "Number of dimension: 4096\n",
      "\n",
      "Training set\n",
      "Number of observations: 5000\n",
      "Number of dimension: 4096\n",
      "\n",
      "Validation set\n",
      "Number of observations: 1000\n",
      "Number of dimension: 4096\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_train = np.load('train_features.npy')\n",
    "y_train = np.load('train_labels.npy')\n",
    "x_val = np.load('val_features.npy')\n",
    "y_val = np.load('val_labels.npy')\n",
    "\n",
    "# Subset training data: classes 0 and 1\n",
    "class1 = 0\n",
    "class2 = 1\n",
    "xtrain = x_train\n",
    "ytrain = y_train\n",
    "idx_train = np.array([np.where(ytrain==class1),np.where(ytrain==class2)]).reshape(-1)\n",
    "xtrain = xtrain[idx_train]\n",
    "ytrain = ytrain[idx_train]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(np.array(xtrain))\n",
    "xtrain = scaler.transform(xtrain).T\n",
    "scaler = scaler.fit(np.array(x_train))\n",
    "x_train = scaler.transform(x_train).T\n",
    "scaler = scaler.fit(np.array(x_val))\n",
    "x_val = scaler.transform(x_val).T\n",
    "ytrain = ytrain.T\n",
    "y_train = y_train.T\n",
    "y_val = y_val.T\n",
    "\n",
    "# Change label to +/- 1\n",
    "ytrain[ytrain!=class2] = -1\n",
    "\n",
    "print('Training subset')\n",
    "print('Number of observations: %d' % xtrain.shape[1])\n",
    "print('Number of dimension: %d' % xtrain.shape[0])\n",
    "print('\\nTraining set')\n",
    "print('Number of observations: %d' % x_train.shape[1])\n",
    "print('Number of dimension: %d' % x_train.shape[0])\n",
    "print('\\nValidation set')\n",
    "print('Number of observations: %d' % x_val.shape[1])\n",
    "print('Number of dimension: %d' % x_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict(X, beta, cls1, cls2, threshold=0.5):\n",
    "    pred = 1/(1+np.exp(-X.T.dot(beta))) > threshold # logistic function\n",
    "    pred = pred.astype(int) # True 1 False 0\n",
    "    pred[pred==0] = cls1\n",
    "    pred[pred==1] = cls2\n",
    "    return pred.T\n",
    "\n",
    "def compME(X, Y, beta):\n",
    "    pred = predict(X, beta, class1, class2)\n",
    "    err = np.mean(pred != Y)\n",
    "    return err\n",
    "\n",
    "def ME_plot(X1, Y1, X2, Y2, vals):\n",
    "    me1 = []\n",
    "    me2 = []\n",
    "    for val in vals:\n",
    "        me1.append(compME(X=X1, Y=Y1, beta=val))\n",
    "        me2.append(compME(X=X2, Y=Y2, beta=val))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(me1, label='Training Set')\n",
    "    plt.plot(me2, label='Validation Set')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Misclassification Error')\n",
    "    plt.title('Misclassification Error', fontsize=13)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train a classifier\n",
    "vals = myrhologistic(X=xtrain, Y=ytrain, init=np.zeros([xtrain.shape[0],]), lamb=1, rho=2, eps=0.001)\n",
    "\n",
    "# Misclassification error plot\n",
    "ME_plot(x_train, y_train, x_val, y_val, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the value of the regularization parameter $\\lambda$ using leave-one-out cross-validation. Find the value of the regularization parameter $\\lambda$ using hold-out cross-validation. Train a classifier using $l_2$-regularized binary logistic regression with $\\rho$-logistic loss on the training set using your own accelerated gradient algorithm with that value of $\\lambda$ found by hold-out cross-validation. Plot, with different colors, the misclassification error on the training set and on the validation set vs. iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leave-one-out cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = -1\n",
    "class2 = 1\n",
    "lamb_opt = crossval(X=xtrain.T, Y=ytrain, option='Leave-one-out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run Leave-one-out cross validation on AWS overnight\n",
    "lamb_opt = 0\n",
    "\n",
    "# Train a classifier\n",
    "vals = myrhologistic(X=xtrain, Y=ytrain, init=np.zeros([xtrain.shape[0],]), lamb=lamb_opt, rho=2, eps=0.001)\n",
    "ME_plot(x_train, y_train, x_val, y_val, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hold-out cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9150000000\n",
      "0.1000   0.9150000000\n",
      "0.2000   0.9250000000\n",
      "0.3000   0.9250000000\n",
      "0.4000   0.9250000000\n",
      "0.5000   0.9250000000\n",
      "0.6000   0.9250000000\n",
      "0.7000   0.9250000000\n",
      "0.8000   0.9250000000\n",
      "0.9000   0.9250000000\n",
      "1.0000   0.9250000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.2000 is 0.0750000000\n"
     ]
    }
   ],
   "source": [
    "class1 = -1\n",
    "class2 = 1\n",
    "lamb_opt = crossval(X=xtrain.T, Y=ytrain, option='Hold-out', train_percent=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train a classifier\n",
    "vals = myrhologistic(X=xtrain, Y=ytrain, init=np.zeros([xtrain.shape[0],]), lamb=lamb_opt, rho=2, eps=0.001)\n",
    "class1 = 0\n",
    "class2 = 1\n",
    "ME_plot(x_train, y_train, x_val, y_val, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider all pairs of classes from the dataset. For each pair of classes, train a classifier using a $l_2$-regularized binary logistic regression with $\\rho$-logistic loss on the training set comprising only the data-points for that pair of classes using your own fast gradient algorithm. For each pair of classes, find the value of the regularization parameter $\\lambda$ using hold-out cross-validation on the training set comprising only the data-points for that pair of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetData(X, Y):\n",
    "    # Subset training data\n",
    "    x = X\n",
    "    y = Y\n",
    "    idx_train = np.array([np.where(y==class1),np.where(y==class2)]).reshape(-1)\n",
    "    x = x[idx_train]\n",
    "    y = y[idx_train]\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler = scaler.fit(np.array(x))\n",
    "    x = scaler.transform(x).T\n",
    "    y = y.T\n",
    "\n",
    "    # Change label to +/- 1\n",
    "    y[y==class1] = -1\n",
    "    y[y==class2] = 1\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 0\n",
      "Class 2: 1\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9150000000\n",
      "0.1000   0.9250000000\n",
      "0.2000   0.9250000000\n",
      "0.3000   0.9250000000\n",
      "0.4000   0.9250000000\n",
      "0.5000   0.9250000000\n",
      "0.6000   0.9250000000\n",
      "0.7000   0.9250000000\n",
      "0.8000   0.9200000000\n",
      "0.9000   0.9250000000\n",
      "1.0000   0.9250000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0750000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 2\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8800000000\n",
      "0.1000   0.8850000000\n",
      "0.2000   0.8900000000\n",
      "0.3000   0.8850000000\n",
      "0.4000   0.8900000000\n",
      "0.5000   0.8900000000\n",
      "0.6000   0.8900000000\n",
      "0.7000   0.8900000000\n",
      "0.8000   0.8850000000\n",
      "0.9000   0.8850000000\n",
      "1.0000   0.8850000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.2000 is 0.1100000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 3\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9400000000\n",
      "0.1000   0.9400000000\n",
      "0.2000   0.9400000000\n",
      "0.3000   0.9450000000\n",
      "0.4000   0.9450000000\n",
      "0.5000   0.9450000000\n",
      "0.6000   0.9450000000\n",
      "0.7000   0.9450000000\n",
      "0.8000   0.9450000000\n",
      "0.9000   0.9450000000\n",
      "1.0000   0.9450000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.3000 is 0.0550000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 4\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9050000000\n",
      "0.1000   0.9050000000\n",
      "0.2000   0.9100000000\n",
      "0.3000   0.9150000000\n",
      "0.4000   0.9150000000\n",
      "0.5000   0.9200000000\n",
      "0.6000   0.9200000000\n",
      "0.7000   0.9200000000\n",
      "0.8000   0.9200000000\n",
      "0.9000   0.9200000000\n",
      "1.0000   0.9200000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.5000 is 0.0800000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 5\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9300000000\n",
      "0.1000   0.9300000000\n",
      "0.2000   0.9300000000\n",
      "0.3000   0.9300000000\n",
      "0.4000   0.9350000000\n",
      "0.5000   0.9350000000\n",
      "0.6000   0.9350000000\n",
      "0.7000   0.9350000000\n",
      "0.8000   0.9400000000\n",
      "0.9000   0.9400000000\n",
      "1.0000   0.9400000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.8000 is 0.0600000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 6\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9350000000\n",
      "0.1000   0.9350000000\n",
      "0.2000   0.9350000000\n",
      "0.3000   0.9350000000\n",
      "0.4000   0.9350000000\n",
      "0.5000   0.9350000000\n",
      "0.6000   0.9350000000\n",
      "0.7000   0.9350000000\n",
      "0.8000   0.9300000000\n",
      "0.9000   0.9300000000\n",
      "1.0000   0.9300000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0650000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9850000000\n",
      "0.1000   0.9850000000\n",
      "0.2000   0.9850000000\n",
      "0.3000   0.9850000000\n",
      "0.4000   0.9850000000\n",
      "0.5000   0.9850000000\n",
      "0.6000   0.9850000000\n",
      "0.7000   0.9850000000\n",
      "0.8000   0.9850000000\n",
      "0.9000   0.9850000000\n",
      "1.0000   0.9850000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0150000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9650000000\n",
      "0.1000   0.9650000000\n",
      "0.2000   0.9650000000\n",
      "0.3000   0.9650000000\n",
      "0.4000   0.9650000000\n",
      "0.5000   0.9650000000\n",
      "0.6000   0.9650000000\n",
      "0.7000   0.9650000000\n",
      "0.8000   0.9650000000\n",
      "0.9000   0.9650000000\n",
      "1.0000   0.9650000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0350000000\n",
      "\n",
      "\n",
      "Class 1: 0\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9800000000\n",
      "0.1000   0.9800000000\n",
      "0.2000   0.9800000000\n",
      "0.3000   0.9800000000\n",
      "0.4000   0.9800000000\n",
      "0.5000   0.9800000000\n",
      "0.6000   0.9800000000\n",
      "0.7000   0.9800000000\n",
      "0.8000   0.9800000000\n",
      "0.9000   0.9800000000\n",
      "1.0000   0.9800000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0200000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 2\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9000000000\n",
      "0.1000   0.9200000000\n",
      "0.2000   0.9200000000\n",
      "0.3000   0.9100000000\n",
      "0.4000   0.9050000000\n",
      "0.5000   0.9050000000\n",
      "0.6000   0.9050000000\n",
      "0.7000   0.9050000000\n",
      "0.8000   0.9050000000\n",
      "0.9000   0.9050000000\n",
      "1.0000   0.9100000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0800000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 3\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8200000000\n",
      "0.1000   0.8250000000\n",
      "0.2000   0.8250000000\n",
      "0.3000   0.8250000000\n",
      "0.4000   0.8300000000\n",
      "0.5000   0.8400000000\n",
      "0.6000   0.8400000000\n",
      "0.7000   0.8400000000\n",
      "0.8000   0.8400000000\n",
      "0.9000   0.8400000000\n",
      "1.0000   0.8400000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.5000 is 0.1600000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 4\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9000000000\n",
      "0.1000   0.9200000000\n",
      "0.2000   0.9200000000\n",
      "0.3000   0.9200000000\n",
      "0.4000   0.9150000000\n",
      "0.5000   0.9150000000\n",
      "0.6000   0.9150000000\n",
      "0.7000   0.9150000000\n",
      "0.8000   0.9100000000\n",
      "0.9000   0.9100000000\n",
      "1.0000   0.9100000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0800000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 5\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9300000000\n",
      "0.1000   0.9350000000\n",
      "0.2000   0.9250000000\n",
      "0.3000   0.9250000000\n",
      "0.4000   0.9250000000\n",
      "0.5000   0.9250000000\n",
      "0.6000   0.9250000000\n",
      "0.7000   0.9300000000\n",
      "0.8000   0.9300000000\n",
      "0.9000   0.9300000000\n",
      "1.0000   0.9300000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0650000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 6\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9250000000\n",
      "0.1000   0.9300000000\n",
      "0.2000   0.9300000000\n",
      "0.3000   0.9300000000\n",
      "0.4000   0.9350000000\n",
      "0.5000   0.9350000000\n",
      "0.6000   0.9350000000\n",
      "0.7000   0.9350000000\n",
      "0.8000   0.9350000000\n",
      "0.9000   0.9350000000\n",
      "1.0000   0.9350000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.4000 is 0.0650000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9550000000\n",
      "0.1000   0.9550000000\n",
      "0.2000   0.9550000000\n",
      "0.3000   0.9550000000\n",
      "0.4000   0.9550000000\n",
      "0.5000   0.9550000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.6000 is 0.0400000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9500000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9650000000\n",
      "0.3000   0.9650000000\n",
      "0.4000   0.9650000000\n",
      "0.5000   0.9650000000\n",
      "0.6000   0.9650000000\n",
      "0.7000   0.9650000000\n",
      "0.8000   0.9650000000\n",
      "0.9000   0.9650000000\n",
      "1.0000   0.9650000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.2000 is 0.0350000000\n",
      "\n",
      "\n",
      "Class 1: 1\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9400000000\n",
      "0.1000   0.9400000000\n",
      "0.2000   0.9400000000\n",
      "0.3000   0.9400000000\n",
      "0.4000   0.9400000000\n",
      "0.5000   0.9400000000\n",
      "0.6000   0.9400000000\n",
      "0.7000   0.9400000000\n",
      "0.8000   0.9400000000\n",
      "0.9000   0.9400000000\n",
      "1.0000   0.9400000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0600000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 3\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8850000000\n",
      "0.1000   0.8750000000\n",
      "0.2000   0.8750000000\n",
      "0.3000   0.8800000000\n",
      "0.4000   0.8800000000\n",
      "0.5000   0.8800000000\n",
      "0.6000   0.8800000000\n",
      "0.7000   0.8800000000\n",
      "0.8000   0.8800000000\n",
      "0.9000   0.8800000000\n",
      "1.0000   0.8800000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.1150000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 4\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8850000000\n",
      "0.1000   0.8950000000\n",
      "0.2000   0.8850000000\n",
      "0.3000   0.8850000000\n",
      "0.4000   0.8900000000\n",
      "0.5000   0.8900000000\n",
      "0.6000   0.8900000000\n",
      "0.7000   0.8900000000\n",
      "0.8000   0.8900000000\n",
      "0.9000   0.8900000000\n",
      "1.0000   0.8900000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.1050000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 5\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8950000000\n",
      "0.1000   0.9050000000\n",
      "0.2000   0.9050000000\n",
      "0.3000   0.9150000000\n",
      "0.4000   0.9150000000\n",
      "0.5000   0.9150000000\n",
      "0.6000   0.9150000000\n",
      "0.7000   0.9100000000\n",
      "0.8000   0.9050000000\n",
      "0.9000   0.9050000000\n",
      "1.0000   0.9050000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.3000 is 0.0850000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 6\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9450000000\n",
      "0.1000   0.9500000000\n",
      "0.2000   0.9500000000\n",
      "0.3000   0.9500000000\n",
      "0.4000   0.9500000000\n",
      "0.5000   0.9500000000\n",
      "0.6000   0.9500000000\n",
      "0.7000   0.9450000000\n",
      "0.8000   0.9450000000\n",
      "0.9000   0.9450000000\n",
      "1.0000   0.9450000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0500000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda   average score\n",
      "0.0000   0.9650000000\n",
      "0.1000   0.9650000000\n",
      "0.2000   0.9600000000\n",
      "0.3000   0.9600000000\n",
      "0.4000   0.9600000000\n",
      "0.5000   0.9600000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0350000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9550000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9600000000\n",
      "0.3000   0.9600000000\n",
      "0.4000   0.9600000000\n",
      "0.5000   0.9600000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0400000000\n",
      "\n",
      "\n",
      "Class 1: 2\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9350000000\n",
      "0.1000   0.9350000000\n",
      "0.2000   0.9350000000\n",
      "0.3000   0.9350000000\n",
      "0.4000   0.9350000000\n",
      "0.5000   0.9350000000\n",
      "0.6000   0.9400000000\n",
      "0.7000   0.9400000000\n",
      "0.8000   0.9400000000\n",
      "0.9000   0.9400000000\n",
      "1.0000   0.9400000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.6000 is 0.0600000000\n",
      "\n",
      "\n",
      "Class 1: 3\n",
      "Class 2: 4\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9050000000\n",
      "0.1000   0.9050000000\n",
      "0.2000   0.9050000000\n",
      "0.3000   0.9050000000\n",
      "0.4000   0.9050000000\n",
      "0.5000   0.9050000000\n",
      "0.6000   0.9050000000\n",
      "0.7000   0.9050000000\n",
      "0.8000   0.9050000000\n",
      "0.9000   0.9050000000\n",
      "1.0000   0.9100000000\n",
      "\n",
      "The misclassification error for optimal lambda = 1.0000 is 0.0900000000\n",
      "\n",
      "\n",
      "Class 1: 3\n",
      "Class 2: 5\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9500000000\n",
      "0.1000   0.9550000000\n",
      "0.2000   0.9550000000\n",
      "0.3000   0.9550000000\n",
      "0.4000   0.9550000000\n",
      "0.5000   0.9550000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.6000 is 0.0400000000\n",
      "\n",
      "\n",
      "Class 1: 3\n",
      "Class 2: 6\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8500000000\n",
      "0.1000   0.8500000000\n",
      "0.2000   0.8350000000\n",
      "0.3000   0.8350000000\n",
      "0.4000   0.8350000000\n",
      "0.5000   0.8350000000\n",
      "0.6000   0.8300000000\n",
      "0.7000   0.8250000000\n",
      "0.8000   0.8250000000\n",
      "0.9000   0.8300000000\n",
      "1.0000   0.8350000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.1500000000\n",
      "\n",
      "\n",
      "Class 1: 3\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9300000000\n",
      "0.1000   0.9450000000\n",
      "0.2000   0.9450000000\n",
      "0.3000   0.9450000000\n",
      "0.4000   0.9450000000\n",
      "0.5000   0.9450000000\n",
      "0.6000   0.9450000000\n",
      "0.7000   0.9450000000\n",
      "0.8000   0.9450000000\n",
      "0.9000   0.9450000000\n",
      "1.0000   0.9450000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0550000000\n",
      "\n",
      "\n",
      "Class 1: 3\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9550000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9600000000\n",
      "0.3000   0.9600000000\n",
      "0.4000   0.9600000000\n",
      "0.5000   0.9600000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0400000000\n",
      "\n",
      "\n",
      "Class 1: 3\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9050000000\n",
      "0.1000   0.9100000000\n",
      "0.2000   0.9100000000\n",
      "0.3000   0.9100000000\n",
      "0.4000   0.9100000000\n",
      "0.5000   0.9100000000\n",
      "0.6000   0.9150000000\n",
      "0.7000   0.9150000000\n",
      "0.8000   0.9100000000\n",
      "0.9000   0.9100000000\n",
      "1.0000   0.9100000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.6000 is 0.0850000000\n",
      "\n",
      "\n",
      "Class 1: 4\n",
      "Class 2: 5\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8800000000\n",
      "0.1000   0.8850000000\n",
      "0.2000   0.8850000000\n",
      "0.3000   0.8850000000\n",
      "0.4000   0.8850000000\n",
      "0.5000   0.8850000000\n",
      "0.6000   0.8850000000\n",
      "0.7000   0.8850000000\n",
      "0.8000   0.8800000000\n",
      "0.9000   0.8700000000\n",
      "1.0000   0.8700000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.1150000000\n",
      "\n",
      "\n",
      "Class 1: 4\n",
      "Class 2: 6\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9650000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9750000000\n",
      "0.3000   0.9750000000\n",
      "0.4000   0.9750000000\n",
      "0.5000   0.9750000000\n",
      "0.6000   0.9750000000\n",
      "0.7000   0.9750000000\n",
      "0.8000   0.9750000000\n",
      "0.9000   0.9750000000\n",
      "1.0000   0.9750000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.2000 is 0.0250000000\n",
      "\n",
      "\n",
      "Class 1: 4\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9800000000\n",
      "0.1000   0.9800000000\n",
      "0.2000   0.9800000000\n",
      "0.3000   0.9800000000\n",
      "0.4000   0.9800000000\n",
      "0.5000   0.9800000000\n",
      "0.6000   0.9800000000\n",
      "0.7000   0.9800000000\n",
      "0.8000   0.9800000000\n",
      "0.9000   0.9800000000\n",
      "1.0000   0.9800000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0200000000\n",
      "\n",
      "\n",
      "Class 1: 4\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9750000000\n",
      "0.1000   0.9700000000\n",
      "0.2000   0.9700000000\n",
      "0.3000   0.9700000000\n",
      "0.4000   0.9700000000\n",
      "0.5000   0.9700000000\n",
      "0.6000   0.9700000000\n",
      "0.7000   0.9700000000\n",
      "0.8000   0.9700000000\n",
      "0.9000   0.9700000000\n",
      "1.0000   0.9700000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0250000000\n",
      "\n",
      "\n",
      "Class 1: 4\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9600000000\n",
      "0.1000   0.9700000000\n",
      "0.2000   0.9700000000\n",
      "0.3000   0.9700000000\n",
      "0.4000   0.9700000000\n",
      "0.5000   0.9700000000\n",
      "0.6000   0.9650000000\n",
      "0.7000   0.9650000000\n",
      "0.8000   0.9650000000\n",
      "0.9000   0.9650000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0300000000\n",
      "\n",
      "\n",
      "Class 1: 5\n",
      "Class 2: 6\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9600000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9600000000\n",
      "0.3000   0.9650000000\n",
      "0.4000   0.9650000000\n",
      "0.5000   0.9650000000\n",
      "0.6000   0.9650000000\n",
      "0.7000   0.9650000000\n",
      "0.8000   0.9650000000\n",
      "0.9000   0.9650000000\n",
      "1.0000   0.9650000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.3000 is 0.0350000000\n",
      "\n",
      "\n",
      "Class 1: 5\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9800000000\n",
      "0.1000   0.9800000000\n",
      "0.2000   0.9800000000\n",
      "0.3000   0.9800000000\n",
      "0.4000   0.9800000000\n",
      "0.5000   0.9800000000\n",
      "0.6000   0.9800000000\n",
      "0.7000   0.9800000000\n",
      "0.8000   0.9800000000\n",
      "0.9000   0.9800000000\n",
      "1.0000   0.9800000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0200000000\n",
      "\n",
      "\n",
      "Class 1: 5\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9750000000\n",
      "0.1000   0.9650000000\n",
      "0.2000   0.9550000000\n",
      "0.3000   0.9550000000\n",
      "0.4000   0.9550000000\n",
      "0.5000   0.9550000000\n",
      "0.6000   0.9550000000\n",
      "0.7000   0.9550000000\n",
      "0.8000   0.9550000000\n",
      "0.9000   0.9550000000\n",
      "1.0000   0.9550000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0250000000\n",
      "\n",
      "\n",
      "Class 1: 5\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9600000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9600000000\n",
      "0.3000   0.9600000000\n",
      "0.4000   0.9600000000\n",
      "0.5000   0.9600000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0400000000\n",
      "\n",
      "\n",
      "Class 1: 6\n",
      "Class 2: 7\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9600000000\n",
      "0.1000   0.9650000000\n",
      "0.2000   0.9700000000\n",
      "0.3000   0.9700000000\n",
      "0.4000   0.9700000000\n",
      "0.5000   0.9700000000\n",
      "0.6000   0.9700000000\n",
      "0.7000   0.9700000000\n",
      "0.8000   0.9700000000\n",
      "0.9000   0.9700000000\n",
      "1.0000   0.9700000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.2000 is 0.0300000000\n",
      "\n",
      "\n",
      "Class 1: 6\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9900000000\n",
      "0.1000   0.9950000000\n",
      "0.2000   0.9950000000\n",
      "0.3000   0.9950000000\n",
      "0.4000   0.9950000000\n",
      "0.5000   0.9950000000\n",
      "0.6000   0.9950000000\n",
      "0.7000   0.9950000000\n",
      "0.8000   0.9950000000\n",
      "0.9000   0.9950000000\n",
      "1.0000   0.9950000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.1000 is 0.0050000000\n",
      "\n",
      "\n",
      "Class 1: 6\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.9600000000\n",
      "0.1000   0.9600000000\n",
      "0.2000   0.9600000000\n",
      "0.3000   0.9600000000\n",
      "0.4000   0.9600000000\n",
      "0.5000   0.9600000000\n",
      "0.6000   0.9600000000\n",
      "0.7000   0.9600000000\n",
      "0.8000   0.9600000000\n",
      "0.9000   0.9600000000\n",
      "1.0000   0.9600000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.0400000000\n",
      "\n",
      "\n",
      "Class 1: 7\n",
      "Class 2: 8\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8250000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1000   0.8450000000\n",
      "0.2000   0.8500000000\n",
      "0.3000   0.8450000000\n",
      "0.4000   0.8450000000\n",
      "0.5000   0.8500000000\n",
      "0.6000   0.8550000000\n",
      "0.7000   0.8500000000\n",
      "0.8000   0.8500000000\n",
      "0.9000   0.8550000000\n",
      "1.0000   0.8500000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.6000 is 0.1450000000\n",
      "\n",
      "\n",
      "Class 1: 7\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8450000000\n",
      "0.1000   0.8500000000\n",
      "0.2000   0.8550000000\n",
      "0.3000   0.8500000000\n",
      "0.4000   0.8450000000\n",
      "0.5000   0.8500000000\n",
      "0.6000   0.8450000000\n",
      "0.7000   0.8500000000\n",
      "0.8000   0.8550000000\n",
      "0.9000   0.8500000000\n",
      "1.0000   0.8500000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.2000 is 0.1450000000\n",
      "\n",
      "\n",
      "Class 1: 8\n",
      "Class 2: 9\n",
      "Hold-out cross-validation\n",
      "lambda   average score\n",
      "0.0000   0.8750000000\n",
      "0.1000   0.8700000000\n",
      "0.2000   0.8700000000\n",
      "0.3000   0.8700000000\n",
      "0.4000   0.8700000000\n",
      "0.5000   0.8700000000\n",
      "0.6000   0.8650000000\n",
      "0.7000   0.8650000000\n",
      "0.8000   0.8700000000\n",
      "0.9000   0.8750000000\n",
      "1.0000   0.8750000000\n",
      "\n",
      "The misclassification error for optimal lambda = 0.0000 is 0.1250000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load train set and validation set\n",
    "x_train = np.load('train_features.npy')\n",
    "y_train = np.load('train_labels.npy').T\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "pairs = list(combinations([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 2))\n",
    "clfs = []\n",
    "for pair in pairs:\n",
    "    class1 = pair[0]\n",
    "    class2 = pair[1]\n",
    "    print('Class 1: %d\\nClass 2: %d' % (class1, class2))\n",
    "    xtrain, ytrain = subsetData(x_train, y_train)\n",
    "    class1 = -1\n",
    "    class2 = 1\n",
    "    # Hold-out cross validation\n",
    "    lamb_opt = crossval(X=xtrain.T, Y=ytrain, option='Hold-out', train_percent=0.8)\n",
    "    print('')\n",
    "    # Train a classifier\n",
    "    vals = myrhologistic(X=xtrain, Y=ytrain, init=np.zeros([xtrain.shape[0],]), lamb=lamb_opt, rho=2, eps=0.001)\n",
    "    clfs.append(vals[vals.shape[0]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function that for any new data point predicts its label. To do this, you will perform the following: input the data point into each classifier (for each pair of classes) you trained above. Record the class predicted by each classifier. Then your prediction for this data point is the most frequently predicted class. If there is a tie, randomly choose between the tied classes. Report the misclassification error on the validation set and test set. Report the precision/recall on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict Label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def predict(X, beta, cls1, cls2, threshold=0.5):\n",
    "    pred = 1/(1+np.exp(-X.T.dot(beta))) > threshold # logistic function\n",
    "    pred = pred.astype(int) # True 1 False 0\n",
    "    pred[pred==0] = cls1\n",
    "    pred[pred==1] = cls2\n",
    "    return pred.T\n",
    "\n",
    "def prediction(X, betas):\n",
    "    preds = np.array([[0]]*X.shape[1])\n",
    "    \n",
    "    for i in range(len(betas)):\n",
    "        pred = predict(X, betas[i], pairs[i][0], pairs[i][1])\n",
    "        pred = np.array([pred])\n",
    "        preds = np.concatenate((preds, pred.T), axis=1)\n",
    "    res, cnt = mode(preds, axis=1)\n",
    "    \n",
    "    return res, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification error on the validation set: 0.3870000000\n",
      "Precision on the validation set: 0.6130000000\n"
     ]
    }
   ],
   "source": [
    "# Load and standardize validation set \n",
    "x_val = np.load('val_features.npy')\n",
    "y_val = np.load('val_labels.npy').T\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(np.array(x_val))\n",
    "x_val = scaler.transform(x_val).T\n",
    "\n",
    "pred, preds = prediction(x_val, clfs)\n",
    "pred = pred.reshape(-1)\n",
    "\n",
    "me_val = np.mean(pred != y_val)\n",
    "print('Misclassification error on the validation set: %.10f' % me_val)\n",
    "print('Precision on the validation set: %.10f' % (1-me_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and standardize test set \n",
    "x_test = np.load('test_features.npy')\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(x_test)\n",
    "x_test = scaler.transform(x_test).T\n",
    "\n",
    "# Predict\n",
    "pred, preds = prediction(x_test, clfs)\n",
    "pred = pred.reshape(-1)\n",
    "\n",
    "# Write to submission.csv\n",
    "df = pd.read_csv('sample_submission.csv')\n",
    "df['Category'] = pred\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification error on the test set: 0.4033400000\n"
     ]
    }
   ],
   "source": [
    "me_test = 1 - 0.59666 # score from Kaggle\n",
    "print('Misclassification error on the test set: %.10f' % me_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
