{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Evaluation Metrics </center>\n",
    "<center> Corinne Jones, TA </center>\n",
    "<center> DATA 558, Spring 2019, University of Washington </center>\n",
    "\n",
    "In this lab we'll discuss evaluation metrics for classification. By the end of this lab, you should be familiar with precision, recall, precision-recall curves, and ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Precision and Recall\n",
    "Two common methods for evaluating the performance of a classifier on a single class are the precision and recall. Precision is defined as \n",
    "$$ \\text{Precision} = \\frac{\\text{# True positives}}{\\text{# True positives + # False positives}} $$\n",
    "and recall is defined as \n",
    "$$ \\text{Recall} = \\frac{\\text{# True positives}}{\\text{# True positives + # False negatives}}. $$\n",
    "Suppose we want to identify whether an image contains a toucan (1) or not (0). Then in this case the true positives are the images that contain toucans, the false positives are the images our classifier labeled as having a toucan that in fact did not have a toucan, and the false negatives are the images our classifier labeled as not having a toucan that did in fact have a toucan.\n",
    "\n",
    "A pictorial description is below. Continuing with the above example, in the picture \"positive images\" would represent the images containing toucans. The \"returned images\" would be the images that our classifier labeled as having a toucan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 886.667x304.444 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps \n",
    "import requests\n",
    "\n",
    "def display_image(url, figsize):\n",
    "    response = requests.get(url)\n",
    "    pca_figure = Image.open(BytesIO(response.content))\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    fig = ax.imshow(np.asarray(pca_figure), aspect='auto')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "url = 'http://www.di.ens.fr/willow/events/cvml2013/materials/practicals/category-level-recognition_files/pr1.png'\n",
    "figsize = (399/45, 137/45)\n",
    "display_image(url, figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** Suppose we classified 10 images, that we know their true labels, and that the results are in the following table:\n",
    "<pre>\n",
    "Image #:         |  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |  9 | 10 |\n",
    "True label:      | +1 | +1 | -1 | -1 | +1 | -1 | +1 | -1 | -1 | -1 |\n",
    "Classifier label:| -1 | +1 | -1 | -1 | +1 | +1 | +1 | +1 | -1 | -1 |\n",
    "</pre>\n",
    "Compute the following:\n",
    "- Number of true positives: 3\n",
    "- Number of true negatives: 4\n",
    "- Number of false positives: 2\n",
    "- Number of false negatives: 1\n",
    "- Precision = 3/(3+2) = 0.6\n",
    "- Recall = 3/(3+1) = 0.75\n",
    "\n",
    "Let's return to the toucan vs. hornbill example from last week. Again, change `data_dir` if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "data_dir = 'lab3_data'\n",
    "\n",
    "# Load the data\n",
    "x_train = np.load(os.path.join(data_dir, 'train_features.npy'))\n",
    "y_train = np.load(os.path.join(data_dir, 'train_labels.npy'))\n",
    "x_test = np.load(os.path.join(data_dir, 'val_features.npy'))\n",
    "y_test = np.load(os.path.join(data_dir, 'val_labels.npy'))\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# Train the classifier\n",
    "classifier = LogisticRegressionCV()\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# Generate predictions based on a threshold of a probability of 0.5\n",
    "yhat_test = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** Compute the precision and recall for the toucan class (without using the sklearn.metrics.precision_score function). Then you can check your code with the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "precision_sklearn = sklearn.metrics.precision_score(y_test, yhat_test)\n",
    "print('Precision according to sklearn = %0.2f' % precision_sklearn)\n",
    "recall_sklearn = sklearn.metrics.recall_score(y_test, yhat_test)\n",
    "print('Recall according to sklearn = %0.2f' % recall_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the images we labeled as having toucans, 91% of them did in fact have a toucan. Out of all of the images with toucans, we only labeled 84% of them as having toucans.\n",
    "\n",
    "## 2 Precision-Recall curve\n",
    "Logistic regression classifiers output a continuous value, and we convert that to $\\pm$ 1 (or 1/0) by applying a threshold. Depending on how you set the threshold, you'll get different precision and recall values. In some applications it's very important to have either high precision or high recall. A classic statistical example of this is when trying to assess whether or not people are guilty of a crime. It's terrible to convict an innocent person of a crime, so we would want to have very few (hopefully zero) false positives and aim for very high precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot a curve of precision and recall values as we change the threshold for what we\n",
    "# use to assign an image to the toucan class vs. the non-toucan (hornbill) class.\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Generate estimated values for test observations using the logistic regression classifier\n",
    "test_probs = classifier.predict_proba(x_test)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, test_probs[:, 1])\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 ROC Curve\n",
    "Another common way to summarize the results is via a receiver operating characteristic (ROC) curve. An ROC curve plots the true positive rate vs. the false positive rate as the threshold is varied. Here we have\n",
    "$$ \\text{True positive rate} = \\text{Recall} = \\frac{\\text{# True positives}}{\\text{# True positives + # False negatives}} $$\n",
    "and\n",
    "$$ \\text{False positive rate} = \\frac{\\text{# False positives}}{\\text{# False positives + # True negatives}}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot a curve of precision and recall values as we change the threshold for what we\n",
    "# use to assign an image to the toucan class vs. the non-toucan (hornbill) class.\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, test_probs[:, 1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes people give a one-number summary of the above plot called the area under the curve (AUC). The AUC is exactly what it sounds like: the area under the curve above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the AUC\n",
    "roc_auc_score(y_test, test_probs[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** Find the value of the threshold that gives a false positive rate of at most 1%. Hint: Use the `fpr` and `thresholds` variables we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When would you use a precision-recall curve vs. an AUC curve? Suppose your data is skewed and that the number of true negatives is huge (compared to the false positives). Then the ROC curve would lie in the very upper left corner of the plot (It doesn't take into account class imbalance). This would be the case in e.g., web search results. However, as the number of true negatives is not taken into account for the precision-recall curve, the precision-recall curve wouldn't necessarily lie in the very upper right part of its plot. On the other hand, if you care about how the model performs on both the positive and negative classes (e.g., classifying toucans vs. hornbills), you're better off using AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
